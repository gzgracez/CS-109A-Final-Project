<!DOCTYPE html>

<html lang="en-us">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge">

  <title>CS109A Introduction to Data Science: Spotify Final Project - SpotifYou</title>
  <link rel="stylesheet" href="http://localhost:4000/assets/css/just-the-docs.css">
  
  <script type="text/javascript" src="http://localhost:4000/assets/js/vendor/lunr.min.js"></script>
  
  <script type="text/javascript" src="http://localhost:4000/assets/js/just-the-docs.js"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
</head>


  <div class="page-wrap">
    <div class="side-bar">
      <a href="http://localhost:4000" class="site-title fs-6 text-grey-dk-300 lh-tight">SpotifYou</a>
      <span class="fs-3"><button class="js-main-nav-trigger navigation-list-toggle btn btn-outline" type="button" data-text-toggle="Hide">Menu</button></span>
      <div class="navigation main-nav js-main-nav">
        <nav>
  <ul class="navigation-list">
  
  
      <li class="navigation-list-item js-side-nav-item">
        
        <a href="http://localhost:4000/" class="navigation-list-link">SpotifYou</a>
          
        
      </li>
  
      <li class="navigation-list-item active js-side-nav-item">
        
        <a href="http://localhost:4000/final_notebook/final_notebook.html" class="navigation-list-link active">CS109A Introduction to Data Science: Spotify Final Project</a>
          
        
      </li>
  
      <li class="navigation-list-item js-side-nav-item">
        
        <a href="http://localhost:4000/milestone-3.html" class="navigation-list-link">Data Collection</a>
          
        
      </li>
  
      <li class="navigation-list-item js-side-nav-item">
        
        <a href="http://localhost:4000/million-playlist-README.html" class="navigation-list-link">The Million Playlist Dataset</a>
          
        
      </li>
  
  </ul>
</nav>

      </div>
      <footer role="contentinfo" class="site-footer">
        <p class="text-small text-grey-dk-000 mb-0">This site uses <a href="https://github.com/pmarsceill/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.</p>
      </footer>
    </div>
    <div class="main-content-wrap">
      <div class="page-header">
        <div class="main-content">
          
          <div class="search js-search">
            <div class="search-input-wrap">
              <input type="text" class="js-search-input search-input" placeholder="Search SpotifYou" aria-label="Search SpotifYou" autocomplete="off">
              <svg width="14" height="14" viewBox="0 0 28 28" xmlns="http://www.w3.org/2000/svg" class="search-icon"><title>Search</title><g fill-rule="nonzero"><path d="M17.332 20.735c-5.537 0-10-4.6-10-10.247 0-5.646 4.463-10.247 10-10.247 5.536 0 10 4.601 10 10.247s-4.464 10.247-10 10.247zm0-4c3.3 0 6-2.783 6-6.247 0-3.463-2.7-6.247-6-6.247s-6 2.784-6 6.247c0 3.464 2.7 6.247 6 6.247z"/><path d="M11.672 13.791L.192 25.271 3.02 28.1 14.5 16.62z"/></g></svg>
            </div>
            <div class="js-search-results search-results-wrap"></div>
          </div>
          
          
        </div>
      </div>
      <div class="main-content">
        
          
        
        <div class="page-content">
          
<h1 id="-cs109a-introduction-to-data-science--spotify-final-project"><img style="float: left; padding-right: 10px; width: 45px" src="https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/iacs.png" /> CS109A Introduction to Data Science:  Spotify Final Project</h1>

<p><strong>Harvard University</strong><br />
<strong>Fall 2018</strong><br />
<strong>Instructors</strong>: Pavlos Protopapas, Kevin Rader<br />
<strong>Group Members</strong>: Tejal Patwardhan, Akshitha Ramachandran, Grace Zhang</p>
<hr style="height:2pt" />

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#RUN THIS CELL </span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">from</span> <span class="nn">IPython.core.display</span> <span class="kn">import</span> <span class="n">HTML</span>
<span class="n">styles</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s">"https://raw.githubusercontent.com/Harvard-IACS/2018-CS109A/master/content/styles/cs109.css"</span><span class="p">)</span><span class="o">.</span><span class="n">text</span>
<span class="n">HTML</span><span class="p">(</span><span class="n">styles</span><span class="p">)</span>
</code></pre></div></div>

<style>
blockquote { background: #AEDE94; }
h1 { 
    padding-top: 25px;
    padding-bottom: 25px;
    text-align: left; 
    padding-left: 10px;
    background-color: #DDDDDD; 
    color: black;
}
h2 { 
    padding-top: 10px;
    padding-bottom: 10px;
    text-align: left; 
    padding-left: 5px;
    background-color: #EEEEEE; 
    color: black;
}

div.exercise {
	background-color: #ffcccc;
	border-color: #E9967A; 	
	border-left: 5px solid #800080; 
	padding: 0.5em;
}
div.theme {
	background-color: #DDDDDD;
	border-color: #E9967A; 	
	border-left: 5px solid #800080; 
	padding: 0.5em;
	font-size: 18pt;
}
div.gc { 
	background-color: #AEDE94;
	border-color: #E9967A; 	 
	border-left: 5px solid #800080; 
	padding: 0.5em;
	font-size: 12pt;
}
p.q1 { 
    padding-top: 5px;
    padding-bottom: 5px;
    text-align: left; 
    padding-left: 5px;
    background-color: #EEEEEE; 
    color: black;
}
header {
   padding-top: 35px;
    padding-bottom: 35px;
    text-align: left; 
    padding-left: 10px;
    background-color: #DDDDDD; 
    color: black;
}
</style>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># import necessary notebooks</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">matplotlib</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="n">plt</span>

<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="n">sm</span>
<span class="kn">from</span> <span class="nn">statsmodels.api</span> <span class="kn">import</span> <span class="n">OLS</span>

<span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">preprocessing</span>
<span class="kn">from</span> <span class="nn">sklearn.utils</span> <span class="kn">import</span> <span class="n">resample</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span><span class="p">,</span> <span class="n">KFold</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegressionCV</span>
<span class="kn">from</span> <span class="nn">sklearn.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">AdaBoostClassifier</span>
<span class="kn">from</span> <span class="nn">sklearn.neighbors</span> <span class="kn">import</span> <span class="n">KNeighborsRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">RidgeCV</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LassoCV</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">accuracy_score</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
<span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span> 
<span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">QuadraticDiscriminantAnalysis</span> 
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">PolynomialFeatures</span>

<span class="kn">from</span> <span class="nn">pandas.plotting</span> <span class="kn">import</span> <span class="n">scatter_matrix</span>

<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s">'whitegrid'</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s">'display.width'</span><span class="p">,</span> <span class="mi">1500</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s">'display.max_columns'</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="kn">import</span> <span class="nn">random</span>

<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>
</code></pre></div></div>

<hr style="height:2pt" />

<h1 id="data-collection-and-cleaning">Data Collection and Cleaning</h1>

<p>We collected our data by using the Spotify API to create a json file of tracks and their features. Additionally, we labeled each track with a new feature , <code class="highlighter-rouge">in_playlist</code>, which equals 1 if Akshitha would include the track in her playlist and 0 if Akshitha would not include the track in her playlist.</p>

<p>We accomplished this by manually creating 2 separate playlists, where one playlist includes random songs that Person X would include in her playlist and the other playlist includes random songs that Person X would not include in her playlist. We used the Spotify API <code class="highlighter-rouge">user_playlist_tracks</code> endpoint to collect some features, including <code class="highlighter-rouge">track_id</code>s, of the tracks in each of these playlists. We then used the <code class="highlighter-rouge">audio_features</code> endpoint of the Spotify API to get additional features like <code class="highlighter-rouge">danceability</code>, etc. for each of our tracks. Finally, we added the <code class="highlighter-rouge">in_playlist</code> feature to each of our tracks and wrote our final object to <code class="highlighter-rouge">spotify.json</code>.</p>

<hr style="height:2pt" />

<h1 id="data-description">Data Description</h1>
<p>Our data includes the following features:</p>
<ul>
  <li><code class="highlighter-rouge">danceability</code>: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.</li>
  <li><code class="highlighter-rouge">energy</code>: Energy represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy. A value of 0.0 is least energetic and 1.0 is most energetic.</li>
  <li><code class="highlighter-rouge">key</code>: The estimated overall key of the track. Integers map to pitches using standard Pitch Class Notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.</li>
  <li><code class="highlighter-rouge">loudness</code>: The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values range between -60 and 0 db.</li>
  <li><code class="highlighter-rouge">mode</code>: Mode represents the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Mode is binary; major is represented by 1 and minor is 0.</li>
  <li><code class="highlighter-rouge">speechiness</code>: Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.</li>
  <li><code class="highlighter-rouge">acousticness</code>: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic. instrumentalness: Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.</li>
  <li><code class="highlighter-rouge">liveness</code>: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.</li>
  <li><code class="highlighter-rouge">valence</code>: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).</li>
  <li><code class="highlighter-rouge">tempo</code>: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.</li>
  <li><code class="highlighter-rouge">duration_ms</code>: The duration of the track in milliseconds.</li>
  <li><code class="highlighter-rouge">time_signature</code>: An estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure).</li>
  <li><code class="highlighter-rouge">popularity</code>: The popularity of a track is a value between 0 and 100, with 100 being the most popular. The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are.Generally speaking, songs that are being played a lot now will have a higher popularity than songs that were played a lot in the past.</li>
  <li><code class="highlighter-rouge">in_playlist</code>: Response variable. Categorical variable for whether in playlist of desire. 1 if in playlist, 0 if not in playlist.</li>
</ul>

<p>The following features were recorded to help with visualization later, but not used as predictors in our analysis, as they are not characteristics of the music itself.</p>
<ul>
  <li><code class="highlighter-rouge">name</code>: Song title</li>
  <li><code class="highlighter-rouge">artist</code>: First artist of song</li>
  <li><code class="highlighter-rouge">type</code>: The object type: “audio_features”</li>
  <li><code class="highlighter-rouge">id</code>: The Spotify ID for the track.</li>
  <li><code class="highlighter-rouge">uri</code>: The Spotify URI for the track.</li>
  <li><code class="highlighter-rouge">track_href</code>: A link to the Web API endpoint providing full details of the track.</li>
  <li><code class="highlighter-rouge">analysis_url</code>: An HTTP URL to access the full audio analysis of this track. An access token is required to access this data.</li>
</ul>

<h1 id="exploratory-data-analysis">Exploratory Data Analysis</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># load in dataset</span>
<span class="n">spotify_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"data/spotify-more2.csv"</span><span class="p">)</span>

<span class="c"># drop unnecessary columns</span>
<span class="n">spotify_df</span> <span class="o">=</span> <span class="n">spotify_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s">'type'</span><span class="p">,</span> <span class="s">'id'</span><span class="p">,</span> <span class="s">'uri'</span><span class="p">,</span> <span class="s">'track_href'</span><span class="p">,</span> <span class="s">'analysis_url'</span><span class="p">,</span> <span class="s">'name'</span><span class="p">,</span> <span class="s">'artist'</span><span class="p">,</span> <span class="s">'Unnamed: 0'</span><span class="p">])</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># display head of data</span>
<span class="n">display</span><span class="p">(</span><span class="n">spotify_df</span><span class="o">.</span><span class="n">head</span><span class="p">())</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>acousticness</th>
      <th>danceability</th>
      <th>duration_ms</th>
      <th>energy</th>
      <th>in_playlist</th>
      <th>instrumentalness</th>
      <th>key</th>
      <th>liveness</th>
      <th>loudness</th>
      <th>mode</th>
      <th>popularity</th>
      <th>speechiness</th>
      <th>tempo</th>
      <th>time_signature</th>
      <th>valence</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.929</td>
      <td>0.516</td>
      <td>138760</td>
      <td>0.0663</td>
      <td>0</td>
      <td>0.000972</td>
      <td>7</td>
      <td>0.1120</td>
      <td>-19.221</td>
      <td>0</td>
      <td>11</td>
      <td>0.0334</td>
      <td>109.879</td>
      <td>4</td>
      <td>0.278</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0.539</td>
      <td>0.454</td>
      <td>324133</td>
      <td>0.2600</td>
      <td>0</td>
      <td>0.000780</td>
      <td>8</td>
      <td>0.0675</td>
      <td>-13.193</td>
      <td>0</td>
      <td>63</td>
      <td>0.0401</td>
      <td>174.322</td>
      <td>5</td>
      <td>0.598</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.360</td>
      <td>0.676</td>
      <td>205773</td>
      <td>0.4400</td>
      <td>0</td>
      <td>0.000069</td>
      <td>0</td>
      <td>0.1620</td>
      <td>-11.960</td>
      <td>1</td>
      <td>59</td>
      <td>0.0291</td>
      <td>80.434</td>
      <td>4</td>
      <td>0.499</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0.984</td>
      <td>0.466</td>
      <td>294307</td>
      <td>0.0718</td>
      <td>0</td>
      <td>0.000931</td>
      <td>0</td>
      <td>0.1070</td>
      <td>-17.999</td>
      <td>1</td>
      <td>56</td>
      <td>0.0374</td>
      <td>121.885</td>
      <td>4</td>
      <td>0.196</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0.779</td>
      <td>0.496</td>
      <td>423573</td>
      <td>0.6340</td>
      <td>0</td>
      <td>0.402000</td>
      <td>5</td>
      <td>0.0746</td>
      <td>-10.328</td>
      <td>0</td>
      <td>60</td>
      <td>0.0364</td>
      <td>93.357</td>
      <td>4</td>
      <td>0.606</td>
    </tr>
  </tbody>
</table>
</div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># display shape of data</span>
<span class="n">display</span><span class="p">(</span><span class="n">spotify_df</span><span class="p">[</span><span class="n">spotify_df</span><span class="p">[</span><span class="s">"in_playlist"</span><span class="p">]</span><span class="o">==</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>(2500, 15)
</code></pre></div></div>

<p>We have 5060 songs in our initial analysis. 2650 are included in Akshitha’s playlist, and 2500 are not included in Akshitha’s playlist.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># generate summary chart of features</span>
<span class="n">features</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">means</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">var</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">ranges</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">mins</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">maxes</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">spotify_df</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">feature</span> <span class="o">!=</span> <span class="s">"in_playlist"</span><span class="p">:</span>
        <span class="n">features</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">feature</span><span class="p">)</span>
        <span class="n">means</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">spotify_df</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">())</span>
        <span class="n">var</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">spotify_df</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span><span class="o">.</span><span class="n">var</span><span class="p">())</span>
        <span class="n">ranges</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">spotify_df</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span><span class="o">.</span><span class="n">ptp</span><span class="p">())</span>
        <span class="n">mins</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">spotify_df</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span><span class="o">.</span><span class="nb">min</span><span class="p">())</span>
        <span class="n">maxes</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">spotify_df</span><span class="p">[</span><span class="n">feature</span><span class="p">]</span><span class="o">.</span><span class="nb">max</span><span class="p">())</span>

<span class="n">summary_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="p">{</span><span class="s">'feature'</span><span class="p">:</span> <span class="n">features</span><span class="p">,</span> 
                                  <span class="s">'mean'</span><span class="p">:</span> <span class="n">means</span><span class="p">,</span>
                                  <span class="s">'var'</span> <span class="p">:</span> <span class="n">var</span><span class="p">,</span>
                                  <span class="s">'range'</span><span class="p">:</span> <span class="n">ranges</span><span class="p">,</span> 
                                  <span class="s">'min'</span><span class="p">:</span> <span class="n">mins</span><span class="p">,</span> 
                                  <span class="s">'max'</span><span class="p">:</span> <span class="n">maxes</span><span class="p">})</span> 
</code></pre></div></div>

<p>Below are summary statistics for all the features we plan to analyze:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">display</span><span class="p">(</span><span class="n">summary_df</span><span class="p">)</span>
</code></pre></div></div>

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>feature</th>
      <th>mean</th>
      <th>var</th>
      <th>range</th>
      <th>min</th>
      <th>max</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>acousticness</td>
      <td>0.540199</td>
      <td>1.267884e-01</td>
      <td>9.959953e-01</td>
      <td>0.000005</td>
      <td>0.996</td>
    </tr>
    <tr>
      <th>1</th>
      <td>danceability</td>
      <td>0.570920</td>
      <td>2.931912e-02</td>
      <td>9.162000e-01</td>
      <td>0.061800</td>
      <td>0.978</td>
    </tr>
    <tr>
      <th>2</th>
      <td>duration_ms</td>
      <td>245718.492885</td>
      <td>1.911563e+10</td>
      <td>3.346533e+06</td>
      <td>44507.000000</td>
      <td>3391040.000</td>
    </tr>
    <tr>
      <th>3</th>
      <td>energy</td>
      <td>0.439224</td>
      <td>6.633419e-02</td>
      <td>9.901450e-01</td>
      <td>0.000855</td>
      <td>0.991</td>
    </tr>
    <tr>
      <th>4</th>
      <td>instrumentalness</td>
      <td>0.143138</td>
      <td>9.302492e-02</td>
      <td>9.870000e-01</td>
      <td>0.000000</td>
      <td>0.987</td>
    </tr>
    <tr>
      <th>5</th>
      <td>key</td>
      <td>5.223913</td>
      <td>1.251578e+01</td>
      <td>1.100000e+01</td>
      <td>0.000000</td>
      <td>11.000</td>
    </tr>
    <tr>
      <th>6</th>
      <td>liveness</td>
      <td>0.163377</td>
      <td>1.798945e-02</td>
      <td>9.800000e-01</td>
      <td>0.012000</td>
      <td>0.992</td>
    </tr>
    <tr>
      <th>7</th>
      <td>loudness</td>
      <td>-10.270219</td>
      <td>3.464989e+01</td>
      <td>4.217600e+01</td>
      <td>-42.476000</td>
      <td>-0.300</td>
    </tr>
    <tr>
      <th>8</th>
      <td>mode</td>
      <td>0.650198</td>
      <td>2.274856e-01</td>
      <td>1.000000e+00</td>
      <td>0.000000</td>
      <td>1.000</td>
    </tr>
    <tr>
      <th>9</th>
      <td>popularity</td>
      <td>36.977470</td>
      <td>4.773025e+02</td>
      <td>1.000000e+02</td>
      <td>0.000000</td>
      <td>100.000</td>
    </tr>
    <tr>
      <th>10</th>
      <td>speechiness</td>
      <td>0.070655</td>
      <td>6.217856e-03</td>
      <td>8.989000e-01</td>
      <td>0.023100</td>
      <td>0.922</td>
    </tr>
    <tr>
      <th>11</th>
      <td>tempo</td>
      <td>117.657563</td>
      <td>8.604272e+02</td>
      <td>1.790410e+02</td>
      <td>42.581000</td>
      <td>221.622</td>
    </tr>
    <tr>
      <th>12</th>
      <td>time_signature</td>
      <td>3.919763</td>
      <td>1.655315e-01</td>
      <td>5.000000e+00</td>
      <td>0.000000</td>
      <td>5.000</td>
    </tr>
    <tr>
      <th>13</th>
      <td>valence</td>
      <td>0.425801</td>
      <td>5.455384e-02</td>
      <td>9.591000e-01</td>
      <td>0.025900</td>
      <td>0.985</td>
    </tr>
  </tbody>
</table>
</div>

<p>We can see that all features have values that are expected as per the Spotify API documentation. To analyze each feature in more granularity we looked at density plots.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">response_col</span> <span class="o">=</span> <span class="s">'in_playlist'</span>
<span class="n">resp_col_loc</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">spotify_df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s">'in_playlist'</span><span class="p">)</span>
<span class="n">spotify_graphs_df</span> <span class="o">=</span> <span class="n">spotify_df</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">response_col</span><span class="p">])</span>
<span class="n">num_cols</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">spotify_graphs_df</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">nbin</span> <span class="o">=</span> <span class="mi">15</span>

<span class="c"># iterate through all the features and display them</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="n">num_cols</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span><span class="mi">50</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_cols</span><span class="p">):</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">spotify_graphs_df</span><span class="p">[</span><span class="n">spotify_df</span><span class="o">.</span><span class="n">in_playlist</span> <span class="o">==</span> <span class="mi">0</span><span class="p">][</span><span class="n">spotify_graphs_df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span> <span class="n">hist</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">kde</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">sns</span><span class="o">.</span><span class="n">distplot</span><span class="p">(</span><span class="n">spotify_graphs_df</span><span class="p">[</span><span class="n">spotify_df</span><span class="o">.</span><span class="n">in_playlist</span> <span class="o">==</span> <span class="mi">1</span><span class="p">][</span><span class="n">spotify_graphs_df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]],</span> <span class="n">hist</span> <span class="o">=</span> <span class="bp">False</span><span class="p">,</span> <span class="n">kde</span> <span class="o">=</span> <span class="bp">True</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">"Density of "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">spotify_graphs_df</span><span class="o">.</span><span class="n">columns</span><span class="p">[</span><span class="n">i</span><span class="p">]))</span>
    <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">r'Frequency'</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">subplots_adjust</span><span class="p">(</span><span class="n">hspace</span><span class="o">=.</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="output_17_0.png" alt="png" /></p>

<p>Looking at the density plots above, we note some features that show clear differences in distribution between the playlist and non-playlist. 
While non-playlist songs contain a roughly uniform distribution of energy values, playlist songs spike at an energy level between 0.2-0.4.
Acousticness in playlist tracks is much higher on average, spiking around 0.8, while non-playlist tracks most frequently have acousticness values around 0.1.
Instrumentalness is a particularly interesting feature. While the distribution non-playlist tracks is bimodal, peaking at around 0 and 0.9, playlist tracks have a few very well-defined peaks between 0 and 0.3. 
We will note in advance that this may induce a risk of overfitting based on instrumentalness values.
Playlist tracks have lower loudnesses on average, centering around -10, while non-playlist tracks -5.
In terms of speechiness, the distribution for playlist tracks has a much lower variance and slightly lower expected value, centering around 0.3 while non-playlist tracks center around 0.4.
Valence for non-playlist tracks is roughly uniformly distributed, while playlist tracks demonstrate a roughly normal distribution centered around 0.3.
Finally in terms of popularity, playlist tracks show a peak in their distribution around 60, while non-playlist tracks have a more variable distribution with a peak between 45-55.
The rest of the features are roughly similar in distribution between playlist and non-playlist tracks.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># pair plots</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">pairplot</span><span class="p">(</span><span class="n">spotify_df</span><span class="p">,</span> <span class="n">hue</span> <span class="o">=</span> <span class="s">"in_playlist"</span><span class="p">,</span> <span class="n">diag_kind</span><span class="o">=</span><span class="s">"kde"</span><span class="p">)</span>
<span class="n">ax</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>/anaconda3/lib/python3.6/site-packages/statsmodels/nonparametric/kde.py:488: RuntimeWarning: invalid value encountered in true_divide
  binned = fast_linbin(X, a, b, gridsize) / (delta * nobs)
/anaconda3/lib/python3.6/site-packages/statsmodels/nonparametric/kdetools.py:34: RuntimeWarning: invalid value encountered in double_scalars
  FAC1 = 2*(np.pi*bw/RANGE)**2
</code></pre></div></div>

<p><img src="output_19_1.png" alt="png" /></p>

<p>The pairplot above demonstrates a few interesting things. First, we notice positive correlations between loudness and energy, loudness and danceability, and danceablility and loudness. All three correlations are weak to moderate. We also notice a negative correlation between acousticness and energy.</p>

<h1 id="baseline-logistic-classifier">Baseline Logistic Classifier</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># set seed</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c"># split into train and test</span>
<span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">spotify_df</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">)</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">response_col</span><span class="p">]),</span> <span class="n">train</span><span class="p">[</span><span class="n">response_col</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">response_col</span><span class="p">]),</span> <span class="n">test</span><span class="p">[</span><span class="n">response_col</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="c"># create logistic model</span>
<span class="n">log_reg_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">log_reg_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c"># predict</span>
<span class="n">log_reg_train_predictions</span> <span class="o">=</span> <span class="n">log_reg_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
<span class="n">log_reg_test_predictions</span> <span class="o">=</span> <span class="n">log_reg_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>

<span class="c"># calculate scores</span>
<span class="n">log_reg_train_score</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">log_reg_train_predictions</span><span class="p">)</span>
<span class="n">log_reg_test_score</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">log_reg_test_predictions</span><span class="p">)</span>

<span class="c"># display scores</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Classification accuracy for train set: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">log_reg_train_score</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Classification accuracy for test set: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">log_reg_test_score</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Classification accuracy for train set: 0.6902173913043478
Classification accuracy for test set: 0.6798418972332015
</code></pre></div></div>

<p>Our baseline logistic model is able to achieve an accuracy of roughly 69% in the training set, and 68% in the test set.</p>

<h1 id="add-quadratic-terms">Add Quadratic Terms</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># add quadratic terms</span>
<span class="n">x_train_q</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">x_test_q</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c"># add quadratic terms</span>
<span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">x_train</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">col</span> <span class="o">!=</span> <span class="s">"mode"</span><span class="p">:</span> <span class="c"># our only binary variable</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">col</span> <span class="o">+</span> <span class="s">"^2"</span> <span class="c"># name column as col^2</span>
        <span class="n">x_train_q</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x_train_q</span><span class="p">[</span><span class="n">col</span><span class="p">])</span>
        <span class="n">x_test_q</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x_test_q</span><span class="p">[</span><span class="n">col</span><span class="p">])</span>

<span class="c"># create logistic model</span>
<span class="n">log_reg_model_q</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">log_reg_model_q</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_q</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c"># predict</span>
<span class="n">log_reg_train_q_predictions</span> <span class="o">=</span> <span class="n">log_reg_model_q</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_train_q</span><span class="p">)</span>
<span class="n">log_reg_test_q_predictions</span> <span class="o">=</span> <span class="n">log_reg_model_q</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test_q</span><span class="p">)</span>

<span class="c"># calculate scores</span>
<span class="n">log_reg_train_q_score</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">log_reg_train_q_predictions</span><span class="p">)</span>
<span class="n">log_reg_test_q_score</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">log_reg_test_q_predictions</span><span class="p">)</span>

<span class="c"># display scores</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Classification accuracy for quadratic terms train set: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">log_reg_train_q_score</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Classification accuracy for quadratic test set: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">log_reg_test_q_score</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Classification accuracy for quadratic terms train set: 0.4943181818181818
Classification accuracy for quadratic test set: 0.49308300395256915
</code></pre></div></div>

<p>When trying to add quadratic terms, we see that the model performs worse. The test and training accuracies are both low at roughly $49.5\%$.</p>

<h1 id="regularization">Regularization</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">alphas</span> <span class="o">=</span> <span class="p">(</span><span class="o">.</span><span class="mi">1</span><span class="p">,</span><span class="o">.</span><span class="mi">5</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">50</span><span class="p">,</span><span class="mi">100</span><span class="p">)</span>

<span class="c"># L1 regularization</span>
<span class="n">lr_l1_model</span> <span class="o">=</span> <span class="n">LogisticRegressionCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s">'l1'</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s">'liblinear'</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">lr_l2_model</span> <span class="o">=</span> <span class="n">LogisticRegressionCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_lr_cv</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">x_train</span><span class="o">=</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="o">=</span><span class="n">y_test</span><span class="p">):</span>
    <span class="n">train_predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
    <span class="n">train_score</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">train_predictions</span><span class="p">)</span>
    <span class="n">test_predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
    <span class="n">test_score</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">test_predictions</span><span class="p">)</span>
    <span class="n">test_confusion_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">test_predictions</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'[{}] Classification accuracy for train set: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">train_score</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'[{}] Classification accuracy for test set: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">test_score</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">train_score</span><span class="p">,</span> <span class="n">test_score</span><span class="p">,</span> <span class="n">test_confusion_matrix</span>
<span class="n">l1_stats</span> <span class="o">=</span> <span class="n">get_lr_cv</span><span class="p">(</span><span class="n">lr_l1_model</span><span class="p">,</span> <span class="s">'L1 Reg'</span><span class="p">)</span>
<span class="n">l2_stats</span> <span class="o">=</span> <span class="n">get_lr_cv</span><span class="p">(</span><span class="n">lr_l2_model</span><span class="p">,</span> <span class="s">'L2 Reg'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[L1 Reg] Classification accuracy for train set: 0.8888339920948617
[L1 Reg] Classification accuracy for test set: 0.8784584980237155
[L2 Reg] Classification accuracy for train set: 0.6902173913043478
[L2 Reg] Classification accuracy for test set: 0.6768774703557312
</code></pre></div></div>

<p>L1 regularization performs much better than L2. The L1 regularized model achieves about 88.9% accuracy in the training data and about 87.8% in the test, well outperforming our baseline model. The L2 regularized model performs on par with our baseline, achieving a training accuracy of around 69% and a test accuracy of 67.7%.</p>

<h1 id="lda-and-qda">LDA and QDA</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># LDA</span>
<span class="n">lda</span> <span class="o">=</span> <span class="n">LinearDiscriminantAnalysis</span><span class="p">()</span>
<span class="n">model_lda</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">acc_lda</span> <span class="o">=</span> <span class="n">model_lda</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">acc_lda_test</span> <span class="o">=</span> <span class="n">model_lda</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="c"># print accuracy scores</span>
<span class="k">print</span><span class="p">(</span><span class="s">"LDA Accuracy, Training Set :"</span><span class="p">,</span><span class="nb">str</span><span class="p">(</span><span class="n">acc_lda</span><span class="p">)</span><span class="o">+</span><span class="s">'</span><span class="si">%</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"LDA Accuracy, Testing Set :"</span><span class="p">,</span><span class="nb">str</span><span class="p">(</span><span class="n">acc_lda_test</span><span class="p">)</span><span class="o">+</span><span class="s">'</span><span class="si">%</span><span class="s">'</span><span class="p">)</span>


<span class="c"># QDA</span>
<span class="n">qda</span> <span class="o">=</span> <span class="n">QuadraticDiscriminantAnalysis</span><span class="p">()</span>
<span class="n">model_qda</span> <span class="o">=</span> <span class="n">qda</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">acc_qda</span> <span class="o">=</span> <span class="n">model_qda</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">acc_qda_test</span> <span class="o">=</span> <span class="n">model_qda</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"QDA Accuracy, Training Set :"</span><span class="p">,</span><span class="nb">str</span><span class="p">(</span><span class="n">acc_qda</span><span class="p">)</span><span class="o">+</span><span class="s">'</span><span class="si">%</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"QDA Accuracy, Testing Set :"</span><span class="p">,</span><span class="nb">str</span><span class="p">(</span><span class="n">acc_qda_test</span><span class="p">)</span><span class="o">+</span><span class="s">'</span><span class="si">%</span><span class="s">'</span><span class="p">)</span>

</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>LDA Accuracy, Training Set : 0.8829051383399209%
LDA Accuracy, Testing Set : 0.8764822134387352%
QDA Accuracy, Training Set : 0.8656126482213439%
QDA Accuracy, Testing Set : 0.8735177865612648%
</code></pre></div></div>

<p>LDA performs better than QDA, and both perform above baseline. LDA achieves an accuracy of about 88.3% in the training and 87.6% in the testing data, while QDA ahieves an accuracy of about 86.6% in the training and 87.3% in the testing data.</p>

<h1 id="decision-trees">Decision Trees</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># classify by depth</span>
<span class="k">def</span> <span class="nf">treeClassifierByDepth</span><span class="p">(</span><span class="n">depth</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cvt</span> <span class="o">=</span> <span class="mi">5</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">depth</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span> <span class="o">=</span> <span class="n">cvt</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 5-fold CV</span>
<span class="n">means</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">lower</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">upper</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">sds</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">trains</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">):</span>
    <span class="c"># fit model</span>
    <span class="n">tc</span> <span class="o">=</span> <span class="n">treeClassifierByDepth</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="c"># calc mean and sd</span>
    <span class="n">cur_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tc</span><span class="p">)</span>
    <span class="n">cur_sd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">tc</span><span class="p">)</span>
    <span class="n">train_val</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    <span class="c"># add to lists</span>
    <span class="n">trains</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_val</span><span class="p">)</span>
    <span class="n">means</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cur_mean</span><span class="p">)</span>
    <span class="n">lower</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cur_mean</span> <span class="o">-</span> <span class="mi">2</span><span class="o">*</span><span class="n">cur_sd</span><span class="p">)</span>
    <span class="n">upper</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cur_mean</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">cur_sd</span><span class="p">)</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">),</span><span class="n">means</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">),</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">"Mean CV score (+/- 2SD)"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">),</span> <span class="n">trains</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Train"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Spotify Playlist Decision Tree Model Estimated Performance"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Maximum Depth"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Score"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="output_36_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># cross validation performance</span>
<span class="n">train_score</span> <span class="o">=</span> <span class="n">means</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Decision Tree Classifier] Mean classification accuracy train: "</span><span class="p">,</span><span class="n">train_score</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Mean +/- 2 SD: ("</span><span class="p">,</span> <span class="n">lower</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span><span class="s">","</span><span class="p">,</span><span class="n">upper</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span><span class="s">")"</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Decision Tree Classifier] Mean classification accuracy (train):  0.8809326430505754
Mean +/- 2 SD: ( 0.877591118434013 , 0.8892112214492387 )
</code></pre></div></div>

<p>We achieve the best cross-validated score at a tree depth of $6$ with an accuracy of 88.1%. Additionally, it had a relatively narrow spread in estimated performances, as there is a roughly 1% difference between +/- two standard deviations.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># test set performance</span>
<span class="n">model_dec_tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">test_score</span> <span class="o">=</span> <span class="n">model_dec_tree</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Decision Tree Classifier] Test set classification accuracy: "</span><span class="p">,</span> <span class="n">test_score</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Decision Tree Classifier] Test set classification accuracy:  0.8952569169960475
</code></pre></div></div>

<p>We see that it performs quite well, with an accuracy score of $0.895$, proving superior to all the other models we have tried so far.</p>

<h1 id="bagging">Bagging</h1>

<p>Create 45 bootstrapped datasets, fitting a decision tree to each of them and saving their predictions:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># bootstrap</span>
<span class="n">new_depth</span> <span class="o">=</span> <span class="mi">6</span>
<span class="n">num_trees</span> <span class="o">=</span> <span class="mi">45</span>
<span class="n">bagging_train_arr</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">bagging_test_arr</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">estimators</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">tree_res</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">new_depth</span><span class="p">)</span>

<span class="c"># classify train and test with bootstrap models</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_trees</span><span class="p">):</span>
    <span class="n">boot_x</span><span class="p">,</span> <span class="n">boot_y</span> <span class="o">=</span> <span class="n">resample</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">fit_tree</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">boot_x</span><span class="p">,</span> <span class="n">boot_y</span><span class="p">)</span>
    <span class="n">estimators</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fit_tree</span><span class="p">)</span>
    <span class="n">bagging_train_arr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_train</span><span class="p">))</span>
    <span class="n">bagging_test_arr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">))</span>
</code></pre></div></div>

<p>Construct dataframs with all the bootstrapped data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># train</span>
<span class="n">bagging_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">bagging_train_arr</span><span class="p">)):</span>
    <span class="n">col_name</span> <span class="o">=</span> <span class="s">"Bootstrap Model "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">bagging_train</span><span class="p">[</span><span class="n">col_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">bagging_train_arr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="c"># test</span>
<span class="n">bagging_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">bagging_test_arr</span><span class="p">)):</span>
    <span class="n">col_name</span> <span class="o">=</span> <span class="s">"Bootstrap Model "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">bagging_test</span><span class="p">[</span><span class="n">col_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">bagging_test_arr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    
<span class="c"># generate renaming row obj</span>
<span class="n">rename</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1104</span><span class="p">):</span>
    <span class="n">rename</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="s">"Training Row "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>


<span class="n">bagging_train</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">rename</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">bagging_test</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">rename</span><span class="p">,</span>  <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>Combine predictions from all the bootstraps and assess how the model performs:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># combining all data points from the data to determine accuracy</span>
<span class="n">y_preds_train</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">y_preds_test</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">bagging_train</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
        <span class="n">y_preds_train</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">y_preds_train</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">bagging_test</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
        <span class="n">y_preds_test</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">y_preds_test</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
<span class="k">def</span> <span class="nf">compare_acc</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">actual</span><span class="p">):</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">preds</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">preds</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">actual</span><span class="o">.</span><span class="n">item</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
            <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span><span class="p">(</span><span class="n">count</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">preds</span><span class="p">))</span>

<span class="n">bagging_train_score</span> <span class="o">=</span> <span class="n">compare_acc</span><span class="p">(</span><span class="n">y_preds_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">bagging_test_score</span> <span class="o">=</span> <span class="n">compare_acc</span><span class="p">(</span><span class="n">y_preds_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"Training Accuracy: "</span><span class="p">,</span> <span class="n">bagging_train_score</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Test Accuracy: "</span><span class="p">,</span> <span class="n">bagging_test_score</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Training Accuracy:  0.9352766798418972
Test Accuracy:  0.9021739130434783
</code></pre></div></div>

<p>The model clearly performed better after using bootstrapped data to fit it. It has increased from 88% on the training data to 93.5%, and from 89.5% on the test data to 90.2%.</p>

<h1 id="random-forest">Random Forest</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># model random forest</span>
<span class="n">model_rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="n">num_trees</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="n">new_depth</span><span class="p">)</span>

<span class="c"># fit model on X_train data</span>
<span class="n">model_rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c"># predict using model</span>
<span class="n">y_pred_train_rf</span> <span class="o">=</span> <span class="n">model_rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
<span class="n">y_pred_test_rf</span> <span class="o">=</span> <span class="n">model_rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>

<span class="c"># accuracy from train and test</span>
<span class="n">train_score_rf</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred_train_rf</span><span class="p">)</span>
<span class="n">test_score_rf</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_test_rf</span><span class="p">)</span>

<span class="c"># print accuracy scores</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Random Forest Accuracy, Training Set :"</span><span class="p">,</span><span class="nb">str</span><span class="p">(</span><span class="n">train_score_rf</span><span class="p">)</span><span class="o">+</span><span class="s">'</span><span class="si">%</span><span class="s">'</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Random Forest Accuracy, Testing Set :"</span><span class="p">,</span><span class="nb">str</span><span class="p">(</span><span class="n">test_score_rf</span><span class="p">)</span><span class="o">+</span><span class="s">'</span><span class="si">%</span><span class="s">'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Random Forest Accuracy, Training Set : 0.9266304347826086%
Random Forest Accuracy, Testing Set : 0.8962450592885376%
</code></pre></div></div>

<p>A random forest, at the same depth as the decision tree (namely a depth of 6) performs well too. The test data reaches an accuracy of about $92.7\%$ in the training at $89.6%$ in the test. Bootstrapping performed better in both the training and test sets.</p>

<h1 id="boosting">Boosting</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># define classifier function</span>
<span class="k">def</span> <span class="nf">boostingClassifier</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">depth</span><span class="p">):</span>
    <span class="c"># AdaBoostClassifier</span>
    <span class="n">abc</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">depth</span><span class="p">),</span>
                         <span class="n">n_estimators</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">)</span>
    <span class="n">abc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="c"># staged_score train to plot</span>
    <span class="n">abc_predicts_train</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">abc</span><span class="o">.</span><span class="n">staged_score</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">abc_predicts_train</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">"train"</span><span class="p">);</span>

    <span class="c"># staged_score test to plot</span>
    <span class="n">abc_predicts_test</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">abc</span><span class="o">.</span><span class="n">staged_score</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">abc_predicts_test</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">"test"</span><span class="p">);</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"AdaBoost Classifier Accuracy, n = "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">depth</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Iterations"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
    <span class="k">return</span><span class="p">(</span><span class="s">"Maximum test accuracy for depth of "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">depth</span><span class="p">)</span><span class="o">+</span><span class="s">" is "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">abc_predicts_test</span><span class="p">))</span><span class="o">+</span><span class="s">" at "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">abc_predicts_test</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">abc_predicts_test</span><span class="p">)))</span><span class="o">+</span><span class="s">" iterations"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">boostingClassifier</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span>
</code></pre></div></div>

<p><img src="output_54_0.png" alt="png" /></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Maximum test accuracy for depth of 1 is 0.9120553359683794 at 390 iterations
</code></pre></div></div>

<p><img src="output_54_2.png" alt="png" /></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Maximum test accuracy for depth of 2 is 0.9347826086956522 at 381 iterations
</code></pre></div></div>

<p><img src="output_54_4.png" alt="png" /></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Maximum test accuracy for depth of 3 is 0.9288537549407114 at 182 iterations
</code></pre></div></div>

<p><img src="output_54_6.png" alt="png" /></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Maximum test accuracy for depth of 4 is 0.9209486166007905 at 229 iterations
</code></pre></div></div>

<p>We see based upon an AdaBoostClassifier the maximum test accuracy of $93.5\%$ is attained at a depth of 2. This is attained after 381 iterations. The AdaBoostClassifier is our best perfoming model so far.</p>

<h1 id="knn">kNN</h1>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># make regressor</span>
<span class="n">ks</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span> <span class="c"># Grid of k's</span>
<span class="n">scores_train</span> <span class="o">=</span> <span class="p">[]</span> <span class="c"># R2 scores</span>
<span class="n">scores_test</span> <span class="o">=</span> <span class="p">[]</span> <span class="c"># R2 scores</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">ks</span><span class="p">:</span>
    <span class="n">knnreg</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">k</span><span class="p">)</span> <span class="c"># Create KNN model</span>
    <span class="n">knnreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="c"># Fit the model to training data</span>
    <span class="n">score_train</span> <span class="o">=</span> <span class="n">knnreg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="c"># Calculate R^2 score</span>
    <span class="n">scores_train</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score_train</span><span class="p">)</span>
    <span class="n">score_test</span> <span class="o">=</span> <span class="n">knnreg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span> <span class="c"># Calculate R^2 score</span>
    <span class="n">scores_test</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">score_test</span><span class="p">)</span>

<span class="c"># Plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ks</span><span class="p">,</span> <span class="n">scores_train</span><span class="p">,</span><span class="s">'o-'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">r'$k$'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">r'$R^{2}$'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">r'Train $R^{2}$'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ks</span><span class="p">,</span> <span class="n">scores_test</span><span class="p">,</span><span class="s">'o-'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">r'$k$'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">r'$R^{2}$'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">r'Test $R^{2}$'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="output_57_0.png" alt="png" /></p>

<p>Our kNN regressor performs quite poorly.</p>


          
        </div>
      </div>
    </div>
  </div>
</html>
