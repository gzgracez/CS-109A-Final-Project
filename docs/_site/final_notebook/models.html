<!DOCTYPE html>

<html lang="en-us">
<head>
  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge">

  <title>Models - Spotify You</title>
  <link rel="stylesheet" href="http://localhost:4000/assets/css/just-the-docs.css">
  
  <script type="text/javascript" src="http://localhost:4000/assets/js/vendor/lunr.min.js"></script>
  
  <script type="text/javascript" src="http://localhost:4000/assets/js/just-the-docs.js"></script>

  <meta name="viewport" content="width=device-width, initial-scale=1">
</head>


  <div class="page-wrap">
    <div class="side-bar">
      <a href="http://localhost:4000" class="site-title fs-6 lh-tight">Spotify You</a>
      <span class="fs-3"><button class="js-main-nav-trigger navigation-list-toggle btn btn-outline" type="button" data-text-toggle="Hide">Menu</button></span>
      <div class="navigation main-nav js-main-nav">
        <nav>
  <ul class="navigation-list">
    
    
      
        
          <li class="navigation-list-item">
            
            <a href="http://localhost:4000/" class="navigation-list-link">Home</a>
            
          </li>
        
      
    
      
        
          <li class="navigation-list-item">
            
            <a href="http://localhost:4000/background.html" class="navigation-list-link">Background</a>
            
          </li>
        
      
    
      
        
          <li class="navigation-list-item">
            
            <a href="http://localhost:4000/final_notebook/data.html" class="navigation-list-link">Data Exploration</a>
            
          </li>
        
      
    
      
        
          <li class="navigation-list-item active">
            
            <a href="http://localhost:4000/final_notebook/models.html" class="navigation-list-link active">Models</a>
            
          </li>
        
      
    
      
        
          <li class="navigation-list-item">
            
            <a href="http://localhost:4000/conclusions.html" class="navigation-list-link">Conclusions</a>
            
          </li>
        
      
    
  </ul>
</nav>

      </div>
      <footer role="contentinfo" class="site-footer">
        <p class="text-small text-grey-dk-000 mb-0">This site uses <a href="https://github.com/pmarsceill/just-the-docs">Just the Docs</a>, a documentation theme for Jekyll.</p>
      </footer>
    </div>
    <div class="main-content-wrap">
      <div class="page-header">
        <div class="main-content">
          
          <div class="search js-search">
            <div class="search-input-wrap">
              <input type="text" class="js-search-input search-input" placeholder="Search Spotify You" aria-label="Search Spotify You" autocomplete="off">
              <svg width="14" height="14" viewBox="0 0 28 28" xmlns="http://www.w3.org/2000/svg" class="search-icon"><title>Search</title><g fill-rule="nonzero"><path d="M17.332 20.735c-5.537 0-10-4.6-10-10.247 0-5.646 4.463-10.247 10-10.247 5.536 0 10 4.601 10 10.247s-4.464 10.247-10 10.247zm0-4c3.3 0 6-2.783 6-6.247 0-3.463-2.7-6.247-6-6.247s-6 2.784-6 6.247c0 3.464 2.7 6.247 6 6.247z"/><path d="M11.672 13.791L.192 25.271 3.02 28.1 14.5 16.62z"/></g></svg>
            </div>
            <div class="js-search-results search-results-wrap"></div>
          </div>
          
          
            <ul class="list-style-none text-small mt-md-1 mb-md-1 pb-4 pb-md-0 js-aux-nav aux-nav">
              
                <li class="d-inline-block my-0"><a href="//github.com/gzgracez/CS-109A-Final-Project">Spotify You on GitHub</a></li>
              
            </ul>
          
        </div>
      </div>
      <div class="main-content">
        
          
        
        <div class="page-content">
          <h1 class="no_toc" id="models">Models</h1>

<h2 class="no_toc text-delta" id="table-of-contents">Table of contents</h2>

<ol id="markdown-toc">
  <li><a href="#setup" id="markdown-toc-setup">Setup</a></li>
  <li><a href="#baseline-model" id="markdown-toc-baseline-model">Baseline Model</a></li>
  <li><a href="#logistic-classifier" id="markdown-toc-logistic-classifier">Logistic Classifier</a></li>
  <li><a href="#logistic-classifier-with-quadratic-terms" id="markdown-toc-logistic-classifier-with-quadratic-terms">Logistic Classifier with Quadratic Terms</a></li>
  <li><a href="#l1-and-l2-regularization" id="markdown-toc-l1-and-l2-regularization">L1 and L2 Regularization</a></li>
  <li><a href="#knn" id="markdown-toc-knn">kNN</a></li>
  <li><a href="#lda-and-qda" id="markdown-toc-lda-and-qda">LDA and QDA</a></li>
  <li><a href="#decision-trees" id="markdown-toc-decision-trees">Decision Trees</a></li>
  <li><a href="#bagging" id="markdown-toc-bagging">Bagging</a></li>
  <li><a href="#random-forest" id="markdown-toc-random-forest">Random Forest</a></li>
  <li><a href="#boosting" id="markdown-toc-boosting">Boosting</a></li>
  <li><a href="#neural-networks" id="markdown-toc-neural-networks">Neural Networks</a></li>
  <li><a href="#model-selection" id="markdown-toc-model-selection">Model Selection</a></li>
</ol>

<hr />

<p>Our goal for this project is to construct a list of songs from a new list of songs that Grace would like to add to her existing playlist. We attempt to do this by training a variety of different models to predict whether or not a song should be included in Graceâ€™s playlist. By framing the question in this way, we recognize that playlist construction can be considered from a classification perspective, where we need to classify each song in our test set as either being <code class="highlighter-rouge">in_playlist</code> or not <code class="highlighter-rouge">in_playlist</code>.</p>

<h1 id="setup">Setup</h1>

<p>We first split our data into a train and test set, so that we are later able to assess how well our model performs in both a train set and a not-seen test set.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">spotify_df</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">response_col</span><span class="p">]),</span> <span class="n">train</span><span class="p">[</span><span class="n">response_col</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">response_col</span><span class="p">]),</span> <span class="n">test</span><span class="p">[</span><span class="n">response_col</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
</code></pre></div></div>

<h1 id="baseline-model">Baseline Model</h1>

<p>We began our project by first constructing a baseline model - one where we simply predict that all songs should be included in our existing playlist. 
This serves as a good source of comparison for our future models, which should at least do better than this trivial one.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">baseline_train_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">y_train</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train</span><span class="p">)</span>
<span class="n">baseline_test_score</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">y_test</span> <span class="o">==</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Baseline model (all songs are added to the existing playlist) train score:'</span><span class="p">,</span> <span class="n">baseline_train_score</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">'Baseline model (all songs are added to the existing playlist) test score:'</span><span class="p">,</span> <span class="n">baseline_test_score</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Baseline model (all songs are added to the existing playlist) train score: 0.5034584980237155
Baseline model (all songs are added to the existing playlist) test score: 0.5158102766798419
</code></pre></div></div>

<p>We can see that our trivial model does not perform very well in either the train or test set, achieving 50.3% accuracy in the train set and 51.6% accuracy in the test set.</p>

<h1 id="logistic-classifier">Logistic Classifier</h1>

<p>Given our objective, we next considered simple, interpretable models that could help us classify our data. 
A clear option was the logistic model, which works well for binary classifications.
We fit a logistic classifier on all 14 features of our training data.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># set seed</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="c"># split into train and test</span>
<span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">spotify_df</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">50</span><span class="p">)</span>
<span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span> <span class="o">=</span> <span class="n">train</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">response_col</span><span class="p">]),</span> <span class="n">train</span><span class="p">[</span><span class="n">response_col</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>
<span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">test</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="n">response_col</span><span class="p">]),</span> <span class="n">test</span><span class="p">[</span><span class="n">response_col</span><span class="p">]</span><span class="o">.</span><span class="n">values</span>

<span class="c"># create logistic model</span>
<span class="n">log_reg_model</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">log_reg_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c"># predict</span>
<span class="n">log_reg_train_predictions</span> <span class="o">=</span> <span class="n">log_reg_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
<span class="n">log_reg_test_predictions</span> <span class="o">=</span> <span class="n">log_reg_model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>

<span class="c"># calculate scores</span>
<span class="n">log_reg_train_score</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">log_reg_train_predictions</span><span class="p">)</span>
<span class="n">log_reg_test_score</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">log_reg_test_predictions</span><span class="p">)</span>

<span class="c"># display scores</span>
<span class="k">print</span><span class="p">(</span><span class="s">'[Logistic Regression] Classification accuracy for train set: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">log_reg_train_score</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'[Logistic Regression] Classification accuracy for test set: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">log_reg_test_score</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Logistic Regression] Classification accuracy for train set: 0.6936758893280632
[Logistic Regression] Classification accuracy for test set: 0.6709486166007905
</code></pre></div></div>

<p>Our baseline logistic model is able to achieve an accuracy of roughly 69.4% in the training set, and 67.1% in the test set. We can see that this is already better than our trivial baseline model, which is a great sign! However, we believe we can build an even better predictive model.</p>

<h1 id="logistic-classifier-with-quadratic-terms">Logistic Classifier with Quadratic Terms</h1>

<p>Our next appraoch was to consider a logistic regression model that includes quadratic terms as well as main effect terms, in an attempt to capture any polynomial relationships that may exist between our features and whether or not a particular song should be included in our playlist.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># add quadratic terms</span>
<span class="n">x_train_q</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">x_test_q</span> <span class="o">=</span> <span class="n">x_test</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

<span class="c"># add quadratic terms</span>
<span class="k">for</span> <span class="n">col</span> <span class="ow">in</span> <span class="n">x_train</span><span class="p">:</span>
    <span class="k">if</span> <span class="n">col</span> <span class="o">!=</span> <span class="s">"mode"</span><span class="p">:</span> <span class="c"># our only binary variable</span>
        <span class="n">name</span> <span class="o">=</span> <span class="n">col</span> <span class="o">+</span> <span class="s">"^2"</span> <span class="c"># name column as col^2</span>
        <span class="n">x_train_q</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x_train_q</span><span class="p">[</span><span class="n">col</span><span class="p">])</span>
        <span class="n">x_test_q</span><span class="p">[</span><span class="n">name</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">x_test_q</span><span class="p">[</span><span class="n">col</span><span class="p">])</span>

<span class="c"># create logistic model</span>
<span class="n">log_reg_model_q</span> <span class="o">=</span> <span class="n">LogisticRegression</span><span class="p">(</span><span class="n">C</span><span class="o">=</span><span class="mi">100000</span><span class="p">,</span> <span class="n">fit_intercept</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">log_reg_model_q</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train_q</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c"># predict</span>
<span class="n">log_reg_train_q_predictions</span> <span class="o">=</span> <span class="n">log_reg_model_q</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_train_q</span><span class="p">)</span>
<span class="n">log_reg_test_q_predictions</span> <span class="o">=</span> <span class="n">log_reg_model_q</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test_q</span><span class="p">)</span>

<span class="c"># calculate scores</span>
<span class="n">log_reg_train_q_score</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">log_reg_train_q_predictions</span><span class="p">)</span>
<span class="n">log_reg_test_q_score</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">log_reg_test_q_predictions</span><span class="p">)</span>

<span class="c"># display scores</span>
<span class="k">print</span><span class="p">(</span><span class="s">'[Logistic Regression With Quadratic Terms] Classification accuracy for train set: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">log_reg_train_q_score</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">'[Logistic Regression With Quadratic Terms] Classification accuracy for test set: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">log_reg_test_q_score</span><span class="p">))</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Logistic Regression With Quadratic Terms] Classification accuracy for train set: 0.4965415019762846
[Logistic Regression With Quadratic Terms] Classification accuracy for test set: 0.4841897233201581
</code></pre></div></div>

<p>However, after adding quadratic terms to our model, we see that the model performs worse. The test and training accuracies are both quite low at roughly 48.4% and 49.7%.</p>

<h1 id="l1-and-l2-regularization">L1 and L2 Regularization</h1>

<p>Given our low scores on our logistic regression model with quadratic terms, we consider adding regularization to our model to make sure that we are not overfitting to our training data. We consider both L1 and L2 regularization.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># L1 regularization</span>
<span class="n">lr_l1_model</span> <span class="o">=</span> <span class="n">LogisticRegressionCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">penalty</span><span class="o">=</span><span class="s">'l1'</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s">'liblinear'</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="c"># L2 regularization</span>
<span class="n">lr_l2_model</span> <span class="o">=</span> <span class="n">LogisticRegressionCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="mi">100000</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_lr_cv</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">model_name</span><span class="p">,</span> <span class="n">x_train</span><span class="o">=</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="o">=</span><span class="n">y_train</span><span class="p">,</span> <span class="n">x_test</span><span class="o">=</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="o">=</span><span class="n">y_test</span><span class="p">):</span>
    <span class="n">train_predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
    <span class="n">train_score</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">train_predictions</span><span class="p">)</span>
    <span class="n">test_predictions</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
    <span class="n">test_score</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">test_predictions</span><span class="p">)</span>
    <span class="n">test_confusion_matrix</span> <span class="o">=</span> <span class="n">confusion_matrix</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">test_predictions</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'[{}] Classification accuracy for train set: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">train_score</span><span class="p">))</span>
    <span class="k">print</span><span class="p">(</span><span class="s">'[{}] Classification accuracy for test set: {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">test_score</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">train_score</span><span class="p">,</span> <span class="n">test_score</span><span class="p">,</span> <span class="n">test_confusion_matrix</span>
<span class="n">l1_stats</span> <span class="o">=</span> <span class="n">get_lr_cv</span><span class="p">(</span><span class="n">lr_l1_model</span><span class="p">,</span> <span class="s">'L1 Reg'</span><span class="p">)</span>
<span class="n">l2_stats</span> <span class="o">=</span> <span class="n">get_lr_cv</span><span class="p">(</span><span class="n">lr_l2_model</span><span class="p">,</span> <span class="s">'L2 Reg'</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[L1 Reg] Classification accuracy for train set: 0.8866106719367589
[L1 Reg] Classification accuracy for test set: 0.8873517786561265
[L2 Reg] Classification accuracy for train set: 0.6926877470355731
[L2 Reg] Classification accuracy for test set: 0.6699604743083004
</code></pre></div></div>

<p>We can see that L1 regularization performs much better than L2. The L1 regularized model achieves about 88.8% accuracy in the training data and about 88.9% in the test, well outperforming our baseline model. The L2 regularized model performs on par with our baseline, achieving a training accuracy of around 69.2% and a test accuracy of 66.9%.</p>

<h1 id="knn">kNN</h1>

<p>We next decided to try a different classification approach, specifically, the k-nearest neighbors model.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># make kNN preds binary</span>
<span class="k">def</span> <span class="nf">parsenKKRes</span><span class="p">(</span><span class="n">predictions</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">predictions</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">0.5</span><span class="p">:</span>
            <span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">predictions</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">predictions</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># make regressor</span>
<span class="n">ks</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span> <span class="c"># Grid of k's</span>
<span class="n">scores_train</span> <span class="o">=</span> <span class="p">[]</span> <span class="c"># R2 scores</span>
<span class="n">scores_test</span> <span class="o">=</span> <span class="p">[]</span> <span class="c"># R2 scores</span>
<span class="n">acc_train</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">acc_test</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">ks</span><span class="p">:</span>
    <span class="n">knnreg</span> <span class="o">=</span> <span class="n">KNeighborsRegressor</span><span class="p">(</span><span class="n">n_neighbors</span><span class="o">=</span><span class="n">k</span><span class="p">)</span> <span class="c"># Create KNN model</span>
    <span class="n">knnreg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span> <span class="c"># Fit the model to training data</span>
    <span class="n">scores_train</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">knnreg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>  <span class="c"># Calculate R^2 score</span>
    <span class="n">scores_test</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">knnreg</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span> <span class="c"># Calculate R^2 score</span>
    <span class="n">predicted_train</span> <span class="o">=</span> <span class="n">knnreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
    <span class="n">predicted_test</span> <span class="o">=</span> <span class="n">knnreg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>
    <span class="n">acc_train</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">parsenKKRes</span><span class="p">(</span><span class="n">predicted_train</span><span class="p">)))</span>
    <span class="n">acc_test</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">parsenKKRes</span><span class="p">(</span><span class="n">predicted_test</span><span class="p">)))</span>

<span class="c"># Plot</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">6</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ks</span><span class="p">,</span> <span class="n">scores_train</span><span class="p">,</span><span class="s">'o-'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">r'$k$'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">r'$R^{2}$'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">r'Train $R^{2}$'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ks</span><span class="p">,</span> <span class="n">scores_test</span><span class="p">,</span><span class="s">'o-'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s">r'$k$'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s">r'$R^{2}$'</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s">r'Test $R^{2}$'</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/final_notebook/output_38_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># determine which k index has best test accuracy</span>
<span class="n">k_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">acc_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[kNN] Classification accuracy for training set: "</span><span class="p">,</span> <span class="n">acc_train</span><span class="p">[</span><span class="n">k_index</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[kNN] Classification accuracy for test set: "</span><span class="p">,</span> <span class="n">acc_test</span><span class="p">[</span><span class="n">k_index</span><span class="p">])</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[kNN] Classification accuracy for training set:  0.6314229249011858
[kNN] Classification accuracy for test set:  0.6590909090909091
</code></pre></div></div>

<p>Our kNN regressor performs at the same level as our baseline logistic classifier. The test set is at a 65.9% accuracy while the training is at 63.1%.</p>

<h1 id="lda-and-qda">LDA and QDA</h1>

<p>We now consider discriminant analysis, which provides an alternative approach to classification.
We will try both LDA and QDA and compare them.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># LDA</span>
<span class="n">lda</span> <span class="o">=</span> <span class="n">LinearDiscriminantAnalysis</span><span class="p">()</span>
<span class="n">model_lda</span> <span class="o">=</span> <span class="n">lda</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">acc_lda</span> <span class="o">=</span> <span class="n">model_lda</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">acc_lda_test</span> <span class="o">=</span> <span class="n">model_lda</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>

<span class="c"># print accuracy scores</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[LDA] Classification accuracy for train set :"</span><span class="p">,</span><span class="n">acc_lda</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[LDA] Classification accuracy for test set :"</span><span class="p">,</span><span class="n">acc_lda_test</span><span class="p">)</span>


<span class="c"># QDA</span>
<span class="n">qda</span> <span class="o">=</span> <span class="n">QuadraticDiscriminantAnalysis</span><span class="p">()</span>
<span class="n">model_qda</span> <span class="o">=</span> <span class="n">qda</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">acc_qda</span> <span class="o">=</span> <span class="n">model_qda</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">acc_qda_test</span> <span class="o">=</span> <span class="n">model_qda</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[QDA] Classification accuracy for train set:"</span><span class="p">,</span><span class="n">acc_qda</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[QDA] Classification accuracy for test set:"</span><span class="p">,</span><span class="n">acc_qda_test</span><span class="p">)</span>

</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[LDA] Classification accuracy for train set : 0.8809288537549407
[LDA] Classification accuracy for test set : 0.8843873517786561
[QDA] Classification accuracy for train set: 0.8656126482213439
[QDA] Classification accuracy for test set: 0.866600790513834
</code></pre></div></div>

<p>LDA performs better than QDA, and both perform above baseline. LDA achieves an accuracy of about 88.1% in the training and 88.4% in the testing data, while QDA ahieves an accuracy of about 86.6% in the training and 86.7% in the testing data.</p>

<h1 id="decision-trees">Decision Trees</h1>
<p>The next type of decision model we are interested in is the decision tree.
We will first create a simple tree.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># classify by depth</span>
<span class="k">def</span> <span class="nf">treeClassifierByDepth</span><span class="p">(</span><span class="n">depth</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cvt</span> <span class="o">=</span> <span class="mi">5</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">depth</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">cv</span> <span class="o">=</span> <span class="n">cvt</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># 5-fold CV</span>
<span class="n">means</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">lower</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">upper</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">sds</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">trains</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">20</span><span class="p">):</span>
    <span class="c"># fit model</span>
    <span class="n">tc</span> <span class="o">=</span> <span class="n">treeClassifierByDepth</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="c"># calc mean and sd</span>
    <span class="n">cur_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">tc</span><span class="p">)</span>
    <span class="n">cur_sd</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">tc</span><span class="p">)</span>
    <span class="n">train_val</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">i</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    <span class="c"># add to lists</span>
    <span class="n">trains</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">train_val</span><span class="p">)</span>
    <span class="n">means</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cur_mean</span><span class="p">)</span>
    <span class="n">lower</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cur_mean</span> <span class="o">-</span> <span class="mi">2</span><span class="o">*</span><span class="n">cur_sd</span><span class="p">)</span>
    <span class="n">upper</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cur_mean</span> <span class="o">+</span> <span class="mi">2</span><span class="o">*</span><span class="n">cur_sd</span><span class="p">)</span>
    
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">),</span><span class="n">means</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">),</span> <span class="n">lower</span><span class="p">,</span> <span class="n">upper</span><span class="p">,</span> <span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">"Mean CV score (+/- 2SD)"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">20</span><span class="p">),</span> <span class="n">trains</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s">"Train"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"Spotify Playlist Decision Tree Model Estimated Performance"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Maximum Depth"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">"Score"</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</code></pre></div></div>

<p><img src="/final_notebook/output_43_0.png" alt="png" /></p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># cross validation performance</span>
<span class="n">train_score</span> <span class="o">=</span> <span class="n">means</span><span class="p">[</span><span class="mi">5</span><span class="p">]</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Decision Tree Classifier] Mean classification accuracy training set: "</span><span class="p">,</span><span class="n">train_score</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"Mean +/- 2 SD: ("</span><span class="p">,</span> <span class="n">lower</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span><span class="s">","</span><span class="p">,</span><span class="n">upper</span><span class="p">[</span><span class="mi">4</span><span class="p">],</span><span class="s">")"</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Decision Tree Classifier] Mean classification accuracy training set:  0.8796923499519297
Mean +/- 2 SD: ( 0.8649746226416641 , 0.8909557288057866 )
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># test set performance</span>
<span class="n">model_dec_tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">test_score</span> <span class="o">=</span> <span class="n">model_dec_tree</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Decision Tree Classifier] Mean classification accuracy test set: "</span><span class="p">,</span> <span class="n">test_score</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Decision Tree Classifier] Mean classification accuracy test set:  0.8903162055335968
</code></pre></div></div>

<p>We achieve the best cross-validation score at a tree depth of 6, with an accuracy of 88.0%. Additionally, we observe a relatively narrow spread in estimated performances, as there is a roughly 2% difference between +/- two standard deviations. We see that this model also performs quite well in the test set, with an accuracy score of 88.7%, proving superior to all the other models we have tried so far.</p>

<h1 id="bagging">Bagging</h1>

<p>Now, we will consider ensemble methods that improve upon our simple decision tree.
The first one we try is bagging: we create 45 bootstrapped datasets, fitting a decision tree to each of them and saving their predictions:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># bootstrap</span>
<span class="n">bagging_train_arr</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">bagging_test_arr</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">estimators</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">tree_res</span> <span class="o">=</span> <span class="p">[]</span>

<span class="n">tree</span> <span class="o">=</span> <span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">new_depth</span><span class="p">)</span>

<span class="c"># classify train and test with bootstrap models</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_trees</span><span class="p">):</span>
    <span class="n">boot_x</span><span class="p">,</span> <span class="n">boot_y</span> <span class="o">=</span> <span class="n">resample</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">fit_tree</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">boot_x</span><span class="p">,</span> <span class="n">boot_y</span><span class="p">)</span>
    <span class="n">estimators</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fit_tree</span><span class="p">)</span>
    <span class="n">bagging_train_arr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_train</span><span class="p">))</span>
    <span class="n">bagging_test_arr</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">))</span>
</code></pre></div></div>

<p>Construct dataframes with all the bootstrapped data:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># train</span>
<span class="n">bagging_train</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">bagging_train_arr</span><span class="p">)):</span>
    <span class="n">col_name</span> <span class="o">=</span> <span class="s">"Bootstrap Model "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">bagging_train</span><span class="p">[</span><span class="n">col_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">bagging_train_arr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>

<span class="c"># test</span>
<span class="n">bagging_test</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">bagging_test_arr</span><span class="p">)):</span>
    <span class="n">col_name</span> <span class="o">=</span> <span class="s">"Bootstrap Model "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">bagging_test</span><span class="p">[</span><span class="n">col_name</span><span class="p">]</span> <span class="o">=</span> <span class="n">bagging_test_arr</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
    
<span class="c"># generate renaming row obj</span>
<span class="n">rename</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1104</span><span class="p">):</span>
    <span class="n">rename</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="s">"Training Row "</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>


<span class="n">bagging_train</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">rename</span><span class="p">,</span> <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
<span class="n">bagging_test</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">rename</span><span class="p">,</span>  <span class="n">inplace</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div>

<p>Combine predictions from all the bootstraps and assess how the model performs:</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># combining all data points from the data to determine accuracy</span>
<span class="n">y_preds_train</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">y_preds_test</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">bagging_train</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
        <span class="n">y_preds_train</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">y_preds_train</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">bagging_test</span><span class="o">.</span><span class="n">iterrows</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">row</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span> <span class="o">&gt;</span> <span class="mf">0.5</span><span class="p">:</span>
        <span class="n">y_preds_test</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">y_preds_test</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        
<span class="k">def</span> <span class="nf">compare_acc</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">actual</span><span class="p">):</span>
    <span class="n">count</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">preds</span><span class="p">)):</span>
        <span class="k">if</span> <span class="n">preds</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">==</span> <span class="n">actual</span><span class="o">.</span><span class="n">item</span><span class="p">(</span><span class="n">i</span><span class="p">):</span>
            <span class="n">count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span><span class="p">(</span><span class="n">count</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">preds</span><span class="p">))</span>

<span class="n">bagging_train_score</span> <span class="o">=</span> <span class="n">compare_acc</span><span class="p">(</span><span class="n">y_preds_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
<span class="n">bagging_test_score</span> <span class="o">=</span> <span class="n">compare_acc</span><span class="p">(</span><span class="n">y_preds_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="s">"[Bagging] Classification accuracy for train set: "</span><span class="p">,</span> <span class="n">bagging_train_score</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Bagging] Classification accuracy for test set: "</span><span class="p">,</span> <span class="n">bagging_test_score</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Bagging] Classification accuracy for train set:  0.9370059288537549
[Bagging] Classification accuracy for test set:  0.9150197628458498
</code></pre></div></div>

<p>The model clearly performed better after using bootstrapped data to fit it. It has increased from 88% on the training data to 94.0%, and from 88.1% on the test data to 90.4%. This makes bagging the most accurate model we have tried so far.</p>

<h1 id="random-forest">Random Forest</h1>

<p>Our next ensemble method is random forest, which randomly subsets predictors upon which to generate decision trees.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># config parameters</span>
<span class="n">num_trees</span> <span class="o">=</span> <span class="mi">45</span>
<span class="n">new_depth</span> <span class="o">=</span> <span class="mi">6</span>

<span class="c"># model random forest</span>
<span class="n">model_rf</span> <span class="o">=</span> <span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="n">num_trees</span><span class="p">,</span> <span class="n">max_depth</span><span class="o">=</span><span class="n">new_depth</span><span class="p">)</span>

<span class="c"># fit model on X_train data</span>
<span class="n">model_rf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c"># predict using model</span>
<span class="n">y_pred_train_rf</span> <span class="o">=</span> <span class="n">model_rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_train</span><span class="p">)</span>
<span class="n">y_pred_test_rf</span> <span class="o">=</span> <span class="n">model_rf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x_test</span><span class="p">)</span>

<span class="c"># accuracy from train and test</span>
<span class="n">train_score_rf</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_train</span><span class="p">,</span> <span class="n">y_pred_train_rf</span><span class="p">)</span>
<span class="n">test_score_rf</span> <span class="o">=</span> <span class="n">accuracy_score</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">y_pred_test_rf</span><span class="p">)</span>

<span class="c"># print accuracy scores</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Random Forest] Classification accuracy for train set: "</span><span class="p">,</span> <span class="n">train_score_rf</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Random Forest] Classification accuracy for test set:"</span><span class="p">,</span> <span class="n">test_score_rf</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Random Forest] Classification accuracy for train set:  0.9300889328063241
[Random Forest] Classification accuracy for test set: 0.9229249011857708
</code></pre></div></div>

<p>A random forest, at the same depth as the decision tree (namely a depth of 6) performs even better. The test data reaches an accuracy of about 92.6% in the training at 91.5% in the test.</p>

<h1 id="boosting">Boosting</h1>
<p>Finally, we will consider boosting, an iterative approach that might eliminate some more of the error in our trees.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># define classifier function</span>
<span class="k">def</span> <span class="nf">boostingClassifier</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">depth</span><span class="p">):</span>
    <span class="c"># AdaBoostClassifier</span>
    <span class="n">abc</span> <span class="o">=</span> <span class="n">AdaBoostClassifier</span><span class="p">(</span><span class="n">DecisionTreeClassifier</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="n">depth</span><span class="p">),</span>
                         <span class="n">n_estimators</span><span class="o">=</span><span class="mi">800</span><span class="p">,</span> <span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">)</span>
    <span class="n">abc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="c"># staged_score train to plot</span>
    <span class="n">abc_predicts_train</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">abc</span><span class="o">.</span><span class="n">staged_score</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">abc_predicts_train</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">"train"</span><span class="p">);</span>

    <span class="c"># staged_score test to plot</span>
    <span class="n">abc_predicts_test</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">abc</span><span class="o">.</span><span class="n">staged_score</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">abc_predicts_test</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s">"test"</span><span class="p">);</span>

    <span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">"AdaBoost Classifier Accuracy, n = "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">depth</span><span class="p">))</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">"Iterations"</span><span class="p">)</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    
    <span class="k">return</span><span class="p">(</span><span class="s">"Maximum test accuracy for depth of "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">depth</span><span class="p">)</span><span class="o">+</span><span class="s">" is "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">abc_predicts_test</span><span class="p">))</span><span class="o">+</span><span class="s">" at "</span><span class="o">+</span><span class="nb">str</span><span class="p">(</span><span class="n">abc_predicts_test</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="nb">max</span><span class="p">(</span><span class="n">abc_predicts_test</span><span class="p">)))</span><span class="o">+</span><span class="s">" iterations"</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">5</span><span class="p">):</span>
    <span class="k">print</span><span class="p">(</span><span class="n">boostingClassifier</span><span class="p">(</span><span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">i</span><span class="p">))</span>
</code></pre></div></div>

<p><img src="/final_notebook/output_60_0.png" alt="png" /></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Maximum test accuracy for depth of 1 is 0.9150197628458498 at 773 iterations
</code></pre></div></div>

<p><img src="/final_notebook/output_60_2.png" alt="png" /></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Maximum test accuracy for depth of 2 is 0.9298418972332015 at 751 iterations
</code></pre></div></div>

<p><img src="/final_notebook/output_60_4.png" alt="png" /></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Maximum test accuracy for depth of 3 is 0.9268774703557312 at 500 iterations
</code></pre></div></div>

<p><img src="/final_notebook/output_60_6.png" alt="png" /></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Maximum test accuracy for depth of 4 is 0.9219367588932806 at 530 iterations
</code></pre></div></div>

<p>We see based upon an AdaBoostClassifier the maximum test accuracy of 93.0% is attained at a depth of 2. This is attained after 751 iterations. The AdaBoostClassifier is our most accurate model so far.</p>

<h1 id="neural-networks">Neural Networks</h1>

<p>Finally, we created an artificial neural network to classify our playlist songs.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># check input and output dimensions</span>
<span class="n">input_dim_2</span> <span class="o">=</span> <span class="n">x_train</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="n">output_dim_2</span> <span class="o">=</span> <span class="mi">1</span>
<span class="k">print</span><span class="p">(</span><span class="n">input_dim_2</span><span class="p">,</span><span class="n">output_dim_2</span><span class="p">)</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>14 1
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># create sequential multi-layer perceptron</span>
<span class="n">model2</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span> 

<span class="c"># initial layer</span>
<span class="n">model2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim_2</span><span class="p">,</span>  
                <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span> 

<span class="c"># second layer</span>
<span class="n">model2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim_2</span><span class="p">,</span>  
                <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>

<span class="c"># third layer</span>
<span class="n">model2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim_2</span><span class="p">,</span>  
                <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span>

<span class="c"># output layer</span>
<span class="n">model2</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">))</span>

<span class="c"># compile the model</span>
<span class="n">model2</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">'sgd'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'accuracy'</span><span class="p">])</span>
<span class="n">model2</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># fit the model</span>
<span class="n">model2_history</span> <span class="o">=</span> <span class="n">model2</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">validation_split</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># model loss</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Neural Net - Model 1] Loss: "</span><span class="p">,</span> <span class="n">model2_history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Neural Net - Model 1] Val Loss: "</span><span class="p">,</span> <span class="n">model2_history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Neural Net - Model 1] Test Loss: "</span><span class="p">,</span> <span class="n">model2</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Neural Net - Model 1] Accuracy: "</span><span class="p">,</span> <span class="n">model2_history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'acc'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Neural Net - Model 1] Val Accuracy: "</span><span class="p">,</span> <span class="n">model2_history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_acc'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Neural Net - Model 1] Loss:  7.79790558079957
[Neural Net - Model 1] Val Loss:  8.034205742033103
[Neural Net - Model 1] Test Loss:  [7.719139232937055, 0.5158102769154334]
[Neural Net - Model 1] Accuracy:  0.5108695654529828
[Neural Net - Model 1] Val Accuracy:  0.49604743024106085
</code></pre></div></div>

<p>Our initial accuracy isnâ€™t great. We achieve an accuracy of 48.9% in the training and 50.4% in the validation, and an accuracy of 48.4% in the test. Letâ€™s see if we can improve our network to fit the data better.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># create sequential multi-layer perceptron</span>
<span class="n">model3</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span> 

<span class="c"># Hidden layers</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">40</span><span class="p">):</span>
    <span class="n">model3</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim_2</span><span class="p">,</span> 
        <span class="n">activation</span><span class="o">=</span><span class="s">'relu'</span><span class="p">))</span> 

<span class="c"># output layer</span>
<span class="n">model3</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="n">output_dim_2</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s">'sigmoid'</span><span class="p">))</span>


<span class="c"># compile the model</span>
<span class="n">model3</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s">'binary_crossentropy'</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s">'adam'</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s">'acc'</span><span class="p">])</span>
<span class="n">model3</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># fit the model</span>
<span class="n">model3_history</span> <span class="o">=</span> <span class="n">model3</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">x_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">validation_split</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="mi">128</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># model loss</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Neural Net - Model 2] Loss: "</span><span class="p">,</span> <span class="n">model3_history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'loss'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Neural Net - Model 2] Val Loss: "</span><span class="p">,</span> <span class="n">model3_history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_loss'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Neural Net - Model 2] Test Loss: "</span><span class="p">,</span> <span class="n">model3</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">x_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">))</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Neural Net - Model 2] Accuracy: "</span><span class="p">,</span> <span class="n">model3_history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'acc'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
<span class="k">print</span><span class="p">(</span><span class="s">"[Neural Net - Model 2] Val Accuracy: "</span><span class="p">,</span> <span class="n">model3_history</span><span class="o">.</span><span class="n">history</span><span class="p">[</span><span class="s">'val_acc'</span><span class="p">][</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
</code></pre></div></div>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[Neural Net - Model 2] Loss:  0.6267417644590524
[Neural Net - Model 2] Val Loss:  0.6291195959220698
[Neural Net - Model 2] Test Loss:  [0.6115785545040026, 0.6432806319398843]
[Neural Net - Model 2] Accuracy:  0.625857809154077
[Neural Net - Model 2] Val Accuracy:  0.6197530875971288
</code></pre></div></div>

<p>Even after changing hyperparameters, our neural network does not perform very well. Using 40 layers and 300 epochs, the accuracy in the training data is still 62.8% while the accuracy in the test is 65.2%. This is baffling, because we expected our neural network to perform very well. Perhaps this mediocre perforance is due to limitations of our data set (only 14 features and &lt;5000 songs), or of the specific methods we used.</p>

<hr />

<h1 id="model-selection">Model Selection</h1>

<p>Based upon the presented analysis, we conclude that our boosted decision tree classifier, at a depth of 2 with 751 iterations, is the best model. It achieves the highest accuracy in the test set, of 93.0%.</p>



          
        </div>
      </div>
    </div>
  </div>
</html>
