{
  "0": {
    "id": "0",
    "title": "Background",
    "content": "Background  Motivation  Approach  Literature ReviewMotivationSpotify is a popular music streaming platform that allows users to curate their own playlists. Music recommender systems help users find songs based on what they might like. We wanted to help Grace curate a Spotify playlist of her favorite songs using the classification techniques we learned in class.Grace struggles to classify with words what makes a song fit into her vibe. “I’d jam to Madison Thompson’s ‘Lonely Together,’ but definitely not to Avicii’s.”When words fail, data science might help. We decided to use techniques from Cs109a to solve the problem of finding the best classification model for Grace’s playlist, and then use that model to find her more songs for her playlist.We had three goals:  to create the best performing model to classify songs as in or out of Grace’s playlist  to use that model to suggest new songs for Grace’s playlist  to produce a sleek interface using jekyll to display our analysis, so that others can replicate our method on their playlistsApproachFirst, we asked Grace to label a set of songs as included in her playlist and another set of songs as songs she would not want to listen to.We used Spotify API to download her playlists with feature data for each song in the playlist.We then randomly split this data into a training and test set. Next, we built and fit a variety of classifiers for Grace’s playlist on the training set. Models we built include:  Baseline Model  Logistic Classifier  Logistic Classifier With Quadratic Terms  Logistic Classifier With L1 Regularization  Logistic Classifier With L2 Regularization  k-Nearest Neighbors  Linear Discriminant Analysis  Quadratic Discriminant Analysis  Decision Tree Classifier  Decision Tree Classifier with Bagging  Random Forest  Decision Tree Classifier with Boosting  Neural NetworkFor each model, we evaluated its accuracy on both our training and and test set. Based on accuracy scores, we determined the classifier with the highest performance on the test set. Finally, we ran our best-performing model on a fresh set of songs and asked Grace if she liked her new playlist.Literature ReviewCurrent Challenges and Visions in Music Recommender Systems Research  Biggest current issues in MRS: cold start, automatic playlist generation, and evaluation.  State-of-the-art techniques in playlist extension include collaborative filtering and Markov chain models          limitations include ordering of songs within playlists and incorporating situational characteristics that affect listeners        Future work includes incorporating personality, current emotional state, political situation, and cultural situation into music recommendations.An Analysis of Approaches Taken in the ACM RecSys Challenge 2018  In 2018, Spotify sponsored a challenge involving addressing the automatic music playlist continuation problem  Most accurate classifiers involved:          Larger training sets produced better-performing models                  If training sets were subset, using a random subset of the playlist rather than the sequentially first songs in a playlist was most accurate in training a model                    Excluding “title” as meta-data for the playlist produced better models across the board      Solutions using the descriptors from the Spotify API were more efficient      TrailMix: An Ensemble Recommender System for Playlist Curation and Continuation  Successful RecSys project TrailMix compared 3 recommender models:          song clustering purely based on title      decorated neural collaborative filtering      decision tree        The title model performed very poorly compared to the other two models          The authors acknowledge that using analysis beyond the literal words, ex. incorporating NLP methods, could help        An ensemble of all three models performed bestWays Explanations Impact End Users’ Mental Models  This paper explored the explanations of music song recommenders to users  Part of enabling users to debug their models is explaining these agents to users well enough for them to build useful mental models  A kNN and bagged decision tree ensemble model was used to recommend songs  Completeness of explanations were more important than soundness in understanding models (ie–explaining everything as opposed to explaining only a part but explaining that part fully correctly)  They found that in general, comprehensive explanations were more useful than simple ones",
    "url": "http://localhost:4000/background.html",
    "relUrl": "/background.html"
  },
  "1": {
    "id": "1",
    "title": "Conclusions",
    "content": "Conclusions  Analysis of Results  Extending Our Model  Limitations and Future WorkAnalysis of ResultsOur best model is the boosted decision tree classifier with a depth of 2 and 751 iterations, which performs with an accuracy of 95.4% in the training set and 93.0% in the test set.The following table summarizes the accuracies for all our models, ordered by accuracy in the test set:            Model Type      Train Accuracy      Test Accuracy                  Naive Model      50.3%      51.6%              Neural Network      62.8%      65.2%              kNN      63.1%      65.9%              Logistic Regression With L2 Regularization      69.2%      66.9%              Baseline Logistic Regression      69.4%      67.1%              QDA      86.6%      86.7%              LDA      88.1%      88.4%              Logistic Regression With L1 Regularization      88.6%      88.7%              Decision Tree Classifier      88.0%      89.0%              Decision Tree Classifier With Bagging      93.6%      91.8%              Random Forest      92.9%      92.0%              Boosted Decision Tree Classifier      95.4%      93.0%      Our lowest performing models include the logistic regression with the naive model, the neural network, and the kNN model.Our best performing models were all ensemble methods. The boosted decision tree classifier, the random forest model, and the decision tree classifier with bagging performed best.We tuned the parameters and hyperparameters of each base model to maximize the accuracy score of each, which leads us to believe that we achieved the maximum possible classification accuracy given the constraints of our dataset.Indeed, this model performs much better than the baseline model, which achieves an accuracy of only 51.6% in the test set.Finally, while usually time and space are considerations when evaluating different types of models, because they do not constrain our original problem, we chose to focus on accuracy. However, a qualitative assessment of these metrics determined that all models were comparable in terms of runtime and memory use with the exception of the neural nets that took additional time.One consideration to note is that the boosted model is less suited for parallelization than other ensemble methods, because it is iterative. However, the runtime was less than a few seconds, so we will prioritize the accuracy of the model over this concern.—Extending Our ModelWe can now try to generate a playlist customized to Grace’s taste using our chosen model. We will present the model with a list of songs that both Grace and the model have not seen before. We’ll then have the model assess whether these songs should be included in the playlist and then verify that with Grace’s opinion.# load in datasetfull_songs_df = pd.read_csv(&quot;data/spotify-test.csv&quot;)# drop unnecessary columnssongs_df = full_songs_df.drop(columns=['type', 'id', 'uri', 'track_href', 'analysis_url', 'name', 'artist', 'Unnamed: 0'])# recreating the best modelbest_abc = AdaBoostClassifier(DecisionTreeClassifier(max_depth=2), n_estimators=800, learning_rate = 0.05)best_abc.fit(x_train, y_train)predictions = best_abc.predict(songs_df)print(&quot;Songs Selected for Grace's Playlist&quot;)for i in range(len(predictions)):    if predictions[i] == 1:        print(full_songs_df.loc[i]['name'])Songs Selected for Grace's PlaylistNever Seen Anything &quot;Quite Like You&quot; - AcousticCrazy World - Live from DublinWhatever You DoCome on Love1,000 YearsMachineAfter DarkSudden Love (Acoustic)GeorgiaI Don't Know About YouThis randomly selected dataset had 26 songs. These songs had never been classified by Grace before and our best model (the boosted decision tree classifier with a depth of 2) was used to predict whether songs would be included in her playlist. We then played all the songs in the dataset to Grace to see whether she would include them in her playlist. The model performed accurately, except for one song which she said she would not have added to her playlist (“I Don’t Know About You”). One reason for this mishap could be that our model isn’t 100% accurate, so this song could be by chance one of the ones it messes up; 1 missed song out of 26 is reasonable for a model with 93% accuracy. Another reason could be that Grace’s actual taste is different from how she made the playlist (perhaps she is in a different emotive or environmental state that temporally affects her preferences, or perhaps her underlying preferences have changed). Despite this error, overall, Grace was pleased that we could use data science to automate her playlist selection procees!Limitations and Future WorkData SizeWe generated a dataset by consolidating a large array of songs that vary in genre, language, tempo, rhythm, etc. We tried to curate a dataset that mimicked the variety of songs that Spotify has. Grace then had to go through these songs and classify whether she would like them in her playlist or not. Due to a multitude of constraints, we only had 5000 songs between both and training and test data. Ideally more songs that accurately capture the variety of songs that Spotify has would improve the training procedures for models.Data InclusionOutside of the side of the data set, there are other data sets that can be used discover new predictors or variables about songs. We can explore lyrics for example and see how that contributes to a model’s recommendations. Thus requires us expanding beyond the SpotifyAPI and exploring other data sets.Adapting PlaylistsThis entire project was built off of the preferences of one individual: Grace. While this proved to be a good proof of concept, future exploration should be done to analyze how the best model can help create playlists for others based upon their interests.Collaborative FilteringCollaborative filtering is another type of data modeling that is commonly used for recommendation algorithms. It is based on the fundamental idea that people perfer things similar to the things they’ve established they like. As such, it would be a good model to further investigate for this given project.Improve Neural NetworkOur neural network did not perform particularly well.While we tuned many hyperparameters, further tuning, exploring other network structures, and changing optimizers may help improve our network.Additionally, we could consider using convolutional neural networks.Dynamic PreferencesFinally, a review of the literature highlighted the importance of dynamic preferences.Individuals often adapt the music they want to listen to at a particular time based on their emotions or external situation.Allowing for a modification of models to include these parameters could be useful.For example, if Grace tracked the time of day that she added each song to her playlist and we could use that time as a feature, we could better suggest songs suited to a particular time of day.This could help adjust for temporal effects, such as desiring certain music during a morning run or commute to work, versus desiring different music for an evening shower, versus desiring different music for a party late at night.",
    "url": "http://localhost:4000/conclusions.html",
    "relUrl": "/conclusions.html"
  },
  "2": {
    "id": "2",
    "title": "Data Exploration",
    "content": "Data Exploration  Data Collection and Cleaning          Playlists      Spotify API        Data Description  Exploratory Data AnalysisData Collection and CleaningPlaylistsGrace created two playlists. The first includes random songs that Grace would include in her playlist (a “favorites” playlist), and the other playlist includes random songs that Grace would not include in her playlist (the “not-so-favorites” playlist). These two playlists are linked below.Grace’s Favorites PlaylistGrace’s Not-So-Favorites PlaylistSpotify APIWe then extracted our data from these playlists by using the Spotify API to create a .csv file of songs and their features. We used the Spotify API’s user_playlist_tracks endpoint to collect track features, including track_ids, of the tracks in each of these playlists. We then used the audio_features endpoint to get additional features like danceability for each of our tracks. Finally, we added the in_playlist feature to each of our tracks, labeling them with a 1 if they were from the favorites playlist and a 0 indicating the not-so-favorites playlist.All of this data was ultimately written to a spotify.csv file.import sysimport spotipyimport spotipy.util as utilimport jsonimport pandas as pdfrom math import ceilscope = 'user-library-read'LIMIT = 50PLAYLIST_1_LEN = 2660PLAYLIST_0_LEN = 2492def get_track_features_offset(playlist_id, offset, in_playlist):    results = sp.user_playlist_tracks(        'UroAv2poQoWSvUOfch8wmg',         playlist_id=playlist_id,        limit=LIMIT,        offset=offset,    )    track_infos = []    for i in results['items']:        track_infos.append({            'id': i['track']['id'],            'name': i['track']['name'],            'popularity': i['track']['popularity'],            'artist': i['track']['artists'][0]['name'] if len(i['track']['artists']) &amp;gt; 0 else None,        })    track_ids = [i['id'] for i in track_infos]    try:        tracks_features = sp.audio_features(track_ids)    except:        return []    for idx, track in enumerate(tracks_features):        track['name'] = track_infos[idx]['name']        track['popularity'] = track_infos[idx]['popularity']        track['artist'] = track_infos[idx]['artist']        track['in_playlist'] = in_playlist    return tracks_featuresdef get_track_features(playlist_id, num_iters, in_playlist):    track_features = []    for i in range(num_iters):        track_features.extend(            get_track_features_offset(playlist_id, i * LIMIT, in_playlist)        )    return track_features# Setupif len(sys.argv) &amp;gt; 1:    username = sys.argv[1]else:    print(&quot;Usage: %s username&quot; % (sys.argv[0],))    sys.exit()token = util.prompt_for_user_token(username, scope)if not token:     print(&quot;Can't get token for&quot;, username)sp = spotipy.Spotify(auth=token)# Get track featuresn_playlist0 = ceil(PLAYLIST_0_LEN / LIMIT)n_playlist1 = ceil(PLAYLIST_1_LEN / LIMIT)tracks_features0 = get_track_features('4B3qR5p6PD8nXXeq4C0Gz7', n_playlist0, 0)tracks_features1 = get_track_features('6Jpt5r9KD8FEUDioBFV0r0', n_playlist1, 1)tracks_features = tracks_features0 + tracks_features1with open('spotify-more2.csv', mode='w') as f:    df = pd.read_json(json.dumps(tracks_features))    f.write(df.to_csv())Data DescriptionOur data includes the following features:  danceability: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.  energy: Energy represents a perceptual measure of intensity and activity. Typically, energetic tracks feel fast, loud, and noisy. For example, death metal has high energy, while a Bach prelude scores low on the scale. Perceptual features contributing to this attribute include dynamic range, perceived loudness, timbre, onset rate, and general entropy. A value of 0.0 is least energetic and 1.0 is most energetic.  key: The estimated overall key of the track. Integers map to pitches using standard Pitch Class Notation. E.g. 0 = C, 1 = C♯/D♭, 2 = D, and so on. If no key was detected, the value is -1.  loudness: The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Loudness is the quality of a sound that is the primary psychological correlate of physical strength (amplitude). Values range between -60 and 0 db.  mode: Mode represents the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Mode is binary; major is represented by 1 and minor is 0.  speechiness: Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording (e.g. talk show, audio book, poetry), the closer to 1.0 the attribute value. Values above 0.66 describe tracks that are probably made entirely of spoken words. Values between 0.33 and 0.66 describe tracks that may contain both music and speech, either in sections or layered, including such cases as rap music. Values below 0.33 most likely represent music and other non-speech-like tracks.  acousticness: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.  instrumentalness: Predicts whether a track contains no vocals. “Ooh” and “aah” sounds are treated as instrumental in this context. Rap or spoken word tracks are clearly “vocal”. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content. Values above 0.5 are intended to represent instrumental tracks, but confidence is higher as the value approaches 1.0.  liveness: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. A value above 0.8 provides strong likelihood that the track is live.  valence: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).  tempo: The overall estimated tempo of a track in beats per minute (BPM). In musical terminology, tempo is the speed or pace of a given piece and derives directly from the average beat duration.  duration_ms: The duration of the track in milliseconds.  time_signature: An estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure).  popularity: The popularity of a track is a value between 0 and 100, with 100 being the most popular. The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are.Generally speaking, songs that are being played a lot now will have a higher popularity than songs that were played a lot in the past.  in_playlist: Response variable. Categorical variable for whether in playlist of desire. 1 if in playlist, 0 if not in playlist.The following features were recorded to help with visualization later, but not used as predictors in our analysis, as they are not characteristics of the music itself.  name: Song title  artist: First artist of song  type: The object type, always deemed ‘audio_features.’  id: The Spotify ID for the track.  uri: The Spotify URI for the track.  track_href: A link to the Web API endpoint providing full details of the track.  analysis_url: An HTTP URL to access the full audio analysis of this track. An access token is required to access this data.Exploratory Data AnalysisWe have 5060 songs in our initial analysis. 2650 are included in Grace’s playlist, and 2500 are not included in Grace’s playlist.Below are summary statistics for all the features we plan to analyze:            feature      mean      var      range      min      max                  acousticness      0.540199      1.267884e-01      9.959953e-01      0.000005      0.996              danceability      0.570920      2.931912e-02      9.162000e-01      0.061800      0.978              duration_ms      245718.492885      1.911563e+10      3.346533e+06      44507.000000      3391040.000              energy      0.439224      6.633419e-02      9.901450e-01      0.000855      0.991              instrumentalness      0.143138      9.302492e-02      9.870000e-01      0.000000      0.987              key      5.223913      1.251578e+01      1.100000e+01      0.000000      11.000              liveness      0.163377      1.798945e-02      9.800000e-01      0.012000      0.992              loudness      -10.270219      3.464989e+01      4.217600e+01      -42.476000      -0.300              mode      0.650198      2.274856e-01      1.000000e+00      0.000000      1.000              popularity      36.977470      4.773025e+02      1.000000e+02      0.000000      100.000              speechiness      0.070655      6.217856e-03      8.989000e-01      0.023100      0.922              tempo      117.657563      8.604272e+02      1.790410e+02      42.581000      221.622              time_signature      3.919763      1.655315e-01      5.000000e+00      0.000000      5.000              valence      0.425801      5.455384e-02      9.591000e-01      0.025900      0.985      We can see that all features have values that are expected as per the Spotify API documentation. To analyze each feature in more granularity we looked at density plots.Looking at the density plots above, we note some features that show clear differences in distribution between the playlist and non-playlist. While non-playlist songs contain a roughly uniform distribution of energy values, playlist songs spike at an energy level between 0.2-0.4.Acousticness in playlist tracks is much higher on average, spiking around 0.8, while non-playlist tracks most frequently have acousticness values around 0.1.Instrumentalness is a particularly interesting feature. While the distribution non-playlist tracks is bimodal, peaking at around 0 and 0.9, playlist tracks have a few very well-defined peaks between 0 and 0.3. We will note in advance that this may induce a risk of overfitting based on instrumentalness values.Playlist tracks have lower loudnesses on average, centering around -10, while non-playlist tracks -5.In terms of speechiness, the distribution for playlist tracks has a much lower variance and slightly lower expected value, centering around 0.3 while non-playlist tracks center around 0.4.Valence for non-playlist tracks is roughly uniformly distributed, while playlist tracks demonstrate a roughly normal distribution centered around 0.3.Finally in terms of popularity, playlist tracks show a peak in their distribution around 60, while non-playlist tracks have a more variable distribution with a peak between 45-55.The rest of the features are roughly similar in distribution between playlist and non-playlist tracks.The pairplot above demonstrates a few interesting things. First, we notice weakly positive correlations between loudness and energy, loudness and danceability, and danceablility and loudness. We also notice a negative correlation between acousticness and energy. These correlations will be useful to keep in mind if we conduct variable selection or regularization at a later point. Also, none of these pairwise plots show clear separability between the two playlists.",
    "url": "http://localhost:4000/final_notebook/data.html",
    "relUrl": "/final_notebook/data.html"
  },
  "3": {
    "id": "3",
    "title": "Home",
    "content": "Spotify You (Group 49)  Overview  Website Navigation  About UsOverviewFor our final project, we created a best-performing model to help classify songs for Grace’s playlist. We used the Spotify API to download features for songs Grace selected as playlist-worthy and playlist-unworthy.After getting a grasp on the data, we built classifiers to predict whether a song should be in or out of her playlist.The Decision Tree Classifier with Boosting was the best-performing model, improving our accuracy in the test set from 51.6% to 93.0%.We used this model to classify a fresh set of songs for Grace, and she loved listening to her augmented playlist!  Website NavigationOur motivations for the project as well as our literature review can be found on the background page. A description of the data as well as our exploratory data analysis can be found on the data exploration page.Our models can be found on the models page. Our results, analysis, conclusions, and suggestions for moving forward can be found on the conclusions page.About UsWe are Tejal Patwardhan, Akshitha Ramachandran, and Grace Zhang, Group 49 for CS109A. Special thanks to Pavlos Protopapas, Kevin Rader, and Rashmi Banthia for their assistance.				",
    "url": "http://localhost:4000/",
    "relUrl": "/"
  },
  "4": {
    "id": "4",
    "title": "Models",
    "content": "Models  Setup  Baseline Model  Logistic Classifier  Logistic Classifier with Quadratic Terms  L1 and L2 Regularization  kNN  LDA and QDA  Decision Trees  Bagging  Random Forest  Boosting  Neural Networks  Model SelectionOur goal for this project is to construct a list of songs from a new list of songs that Grace would like to add to her existing playlist. We attempt to do this by training a variety of different models to predict whether or not a song should be included in Grace’s playlist. By framing the question in this way, we recognize that playlist construction can be considered from a classification perspective, where we need to classify each song in our test set as either being in_playlist or not in_playlist.SetupWe first split our data into a train and test set, so that we are later able to assess how well our model performs in both a train set and a not-seen test set.train, test = train_test_split(spotify_df, test_size = 0.2, random_state=50)x_train, y_train = train.drop(columns=[response_col]), train[response_col].valuesx_test, y_test = test.drop(columns=[response_col]), test[response_col].valuesBaseline ModelWe began our project by first constructing a baseline model - one where we simply predict that all songs should be included in our existing playlist. This serves as a good source of comparison for our future models, which should at least do better than this trivial one.baseline_train_score = np.sum(y_train == 1) / len(train)baseline_test_score = np.sum(y_test == 1) / len(test)print('Baseline model (all songs are added to the existing playlist) train score:', baseline_train_score)print('Baseline model (all songs are added to the existing playlist) test score:', baseline_test_score)Baseline model (all songs are added to the existing playlist) train score: 0.5034584980237155Baseline model (all songs are added to the existing playlist) test score: 0.5158102766798419We can see that our trivial model does not perform very well in either the train or test set, achieving 50.3% accuracy in the train set and 51.6% accuracy in the test set.Logistic ClassifierGiven our objective, we next considered simple, interpretable models that could help us classify our data. A clear option was the logistic model, which works well for binary classifications.We fit a logistic classifier on all 14 features of our training data.# set seedrandom.seed(1)# split into train and testtrain, test = train_test_split(spotify_df, test_size = 0.2, random_state=50)x_train, y_train = train.drop(columns=[response_col]), train[response_col].valuesx_test, y_test = test.drop(columns=[response_col]), test[response_col].values# create logistic modellog_reg_model = LogisticRegression(C=100000, fit_intercept=False)log_reg_model.fit(x_train, y_train)# predictlog_reg_train_predictions = log_reg_model.predict(x_train)log_reg_test_predictions = log_reg_model.predict(x_test)# calculate scoreslog_reg_train_score = accuracy_score(y_train, log_reg_train_predictions)log_reg_test_score = accuracy_score(y_test, log_reg_test_predictions)# display scoresprint('[Logistic Regression] Classification accuracy for train set: {}'.format(log_reg_train_score))print('[Logistic Regression] Classification accuracy for test set: {}'.format(log_reg_test_score))[Logistic Regression] Classification accuracy for train set: 0.6936758893280632[Logistic Regression] Classification accuracy for test set: 0.6709486166007905Our baseline logistic model is able to achieve an accuracy of roughly 69.4% in the training set, and 67.1% in the test set. We can see that this is already better than our trivial baseline model, which is a great sign! However, we believe we can build an even better predictive model.Logistic Classifier with Quadratic TermsOur next appraoch was to consider a logistic regression model that includes quadratic terms as well as main effect terms, in an attempt to capture any polynomial relationships that may exist between our features and whether or not a particular song should be included in our playlist.# add quadratic termsx_train_q = x_train.copy()x_test_q = x_test.copy()# add quadratic termsfor col in x_train:    if col != &quot;mode&quot;: # our only binary variable        name = col + &quot;^2&quot; # name column as col^2        x_train_q[name] = np.square(x_train_q[col])        x_test_q[name] = np.square(x_test_q[col])# create logistic modellog_reg_model_q = LogisticRegression(C=100000, fit_intercept=False)log_reg_model_q.fit(x_train_q, y_train)# predictlog_reg_train_q_predictions = log_reg_model_q.predict(x_train_q)log_reg_test_q_predictions = log_reg_model_q.predict(x_test_q)# calculate scoreslog_reg_train_q_score = accuracy_score(y_train, log_reg_train_q_predictions)log_reg_test_q_score = accuracy_score(y_test, log_reg_test_q_predictions)# display scoresprint('[Logistic Regression With Quadratic Terms] Classification accuracy for train set: {}'.format(log_reg_train_q_score))print('[Logistic Regression With Quadratic Terms] Classification accuracy for test set: {}'.format(log_reg_test_q_score))[Logistic Regression With Quadratic Terms] Classification accuracy for train set: 0.4965415019762846[Logistic Regression With Quadratic Terms] Classification accuracy for test set: 0.4841897233201581However, after adding quadratic terms to our model, we see that the model performs worse. The test and training accuracies are both quite low at roughly 48.4% and 49.7%.L1 and L2 RegularizationGiven our low scores on our logistic regression model with quadratic terms, we consider adding regularization to our model to make sure that we are not overfitting to our training data. We consider both L1 and L2 regularization.# L1 regularizationlr_l1_model = LogisticRegressionCV(cv=5, penalty='l1', solver='liblinear', max_iter=100000).fit(x_train, y_train)# L2 regularizationlr_l2_model = LogisticRegressionCV(cv=5, max_iter=100000).fit(x_train, y_train)def get_lr_cv(model, model_name, x_train=x_train, y_train=y_train, x_test=x_test, y_test=y_test):    train_predictions = model.predict(x_train)    train_score = accuracy_score(y_train, train_predictions)    test_predictions = model.predict(x_test)    test_score = accuracy_score(y_test, test_predictions)    test_confusion_matrix = confusion_matrix(y_test, test_predictions)    print('[{}] Classification accuracy for train set: {}'.format(model_name, train_score))    print('[{}] Classification accuracy for test set: {}'.format(model_name, test_score))    return train_score, test_score, test_confusion_matrixl1_stats = get_lr_cv(lr_l1_model, 'L1 Reg')l2_stats = get_lr_cv(lr_l2_model, 'L2 Reg')[L1 Reg] Classification accuracy for train set: 0.8866106719367589[L1 Reg] Classification accuracy for test set: 0.8873517786561265[L2 Reg] Classification accuracy for train set: 0.6926877470355731[L2 Reg] Classification accuracy for test set: 0.6699604743083004We can see that L1 regularization performs much better than L2. The L1 regularized model achieves about 88.8% accuracy in the training data and about 88.9% in the test, well outperforming our baseline model. The L2 regularized model performs on par with our baseline, achieving a training accuracy of around 69.2% and a test accuracy of 66.9%.kNNWe next decided to try a different classification approach, specifically, the k-nearest neighbors model.# make kNN preds binarydef parsenKKRes(predictions):    for i in range(len(predictions)):        if predictions[i] &amp;lt; 0.5:            predictions[i] = 0        else:            predictions[i] = 1    return predictions# make regressorks = range(1, 100) # Grid of k'sscores_train = [] # R2 scoresscores_test = [] # R2 scoresacc_train = []acc_test = []for k in ks:    knnreg = KNeighborsRegressor(n_neighbors=k) # Create KNN model    knnreg.fit(x_train, y_train) # Fit the model to training data    scores_train.append(knnreg.score(x_train, y_train))  # Calculate R^2 score    scores_test.append(knnreg.score(x_test, y_test)) # Calculate R^2 score    predicted_train = knnreg.predict(x_train)    predicted_test = knnreg.predict(x_test)    acc_train.append(accuracy_score(y_train, parsenKKRes(predicted_train)))    acc_test.append(accuracy_score(y_test, parsenKKRes(predicted_test)))# Plotfig, ax = plt.subplots(1,2, figsize=(20,6))ax[0].plot(ks, scores_train,'o-')ax[0].set_xlabel(r'$k$')ax[0].set_ylabel(r'$R^{2}$')ax[0].set_title(r'Train $R^{2}$')ax[1].plot(ks, scores_test,'o-')ax[1].set_xlabel(r'$k$')ax[1].set_ylabel(r'$R^{2}$')ax[1].set_title(r'Test $R^{2}$')plt.show()# determine which k index has best test accuracyk_index = np.argmax(acc_test)print(&quot;[kNN] Classification accuracy for training set: &quot;, acc_train[k_index])print(&quot;[kNN] Classification accuracy for test set: &quot;, acc_test[k_index])[kNN] Classification accuracy for training set:  0.6314229249011858[kNN] Classification accuracy for test set:  0.6590909090909091Our kNN regressor performs at the same level as our baseline logistic classifier. The test set is at a 65.9% accuracy while the training is at 63.1%.LDA and QDAWe now consider discriminant analysis, which provides an alternative approach to classification.We will try both LDA and QDA and compare them.# LDAlda = LinearDiscriminantAnalysis()model_lda = lda.fit(x_train, y_train)acc_lda = model_lda.score(x_train, y_train)acc_lda_test = model_lda.score(x_test, y_test)# print accuracy scoresprint(&quot;[LDA] Classification accuracy for train set :&quot;,acc_lda)print(&quot;[LDA] Classification accuracy for test set :&quot;,acc_lda_test)# QDAqda = QuadraticDiscriminantAnalysis()model_qda = qda.fit(x_train, y_train)acc_qda = model_qda.score(x_train, y_train)acc_qda_test = model_qda.score(x_test, y_test)print(&quot;[QDA] Classification accuracy for train set:&quot;,acc_qda)print(&quot;[QDA] Classification accuracy for test set:&quot;,acc_qda_test)[LDA] Classification accuracy for train set : 0.8809288537549407[LDA] Classification accuracy for test set : 0.8843873517786561[QDA] Classification accuracy for train set: 0.8656126482213439[QDA] Classification accuracy for test set: 0.866600790513834LDA performs better than QDA, and both perform above baseline. LDA achieves an accuracy of about 88.1% in the training and 88.4% in the testing data, while QDA ahieves an accuracy of about 86.6% in the training and 86.7% in the testing data.Decision TreesThe next type of decision model we are interested in is the decision tree.We will first create a simple tree.# classify by depthdef treeClassifierByDepth(depth, x_train, y_train, cvt = 5):    model = DecisionTreeClassifier(max_depth=depth).fit(x_train, y_train)    return cross_val_score(model, x_train, y_train, cv = cvt)# 5-fold CVmeans = []lower = []upper = []sds = []trains = []for i in range(1, 20):    # fit model    tc = treeClassifierByDepth(i, x_train, y_train)    # calc mean and sd    cur_mean = np.mean(tc)    cur_sd = np.std(tc)    train_val = DecisionTreeClassifier(max_depth=i).fit(x_train, y_train).score(x_train,y_train)    # add to lists    trains.append(train_val)    means.append(cur_mean)    lower.append(cur_mean - 2*cur_sd)    upper.append(cur_mean + 2*cur_sd)    plt.plot(range(1,20),means)plt.fill_between(range(1,20), lower, upper, alpha = 0.3, label = &quot;Mean CV score (+/- 2SD)&quot;)plt.plot(range(1,20), trains, label=&quot;Train&quot;)plt.title(&quot;Spotify Playlist Decision Tree Model Estimated Performance&quot;)plt.xlabel(&quot;Maximum Depth&quot;)plt.ylabel(&quot;Score&quot;)plt.legend()plt.show()# cross validation performancetrain_score = means[5]print(&quot;[Decision Tree Classifier] Mean classification accuracy training set: &quot;,train_score)print(&quot;Mean +/- 2 SD: (&quot;, lower[4],&quot;,&quot;,upper[4],&quot;)&quot;)[Decision Tree Classifier] Mean classification accuracy training set:  0.8796923499519297Mean +/- 2 SD: ( 0.8649746226416641 , 0.8909557288057866 )# test set performancemodel_dec_tree = DecisionTreeClassifier(max_depth=6).fit(x_train, y_train)test_score = model_dec_tree.score(x_test, y_test)print(&quot;[Decision Tree Classifier] Mean classification accuracy test set: &quot;, test_score)[Decision Tree Classifier] Mean classification accuracy test set:  0.8903162055335968We achieve the best cross-validation score at a tree depth of 6, with an accuracy of 88.0%. Additionally, we observe a relatively narrow spread in estimated performances, as there is a roughly 2% difference between +/- two standard deviations. We see that this model also performs quite well in the test set, with an accuracy score of 88.7%, proving superior to all the other models we have tried so far.BaggingNow, we will consider ensemble methods that improve upon our simple decision tree.The first one we try is bagging: we create 45 bootstrapped datasets, fitting a decision tree to each of them and saving their predictions:# bootstrapbagging_train_arr = []bagging_test_arr = []estimators = []tree_res = []tree = DecisionTreeClassifier(max_depth=new_depth)# classify train and test with bootstrap modelsfor i in range(num_trees):    boot_x, boot_y = resample(x_train, y_train)    fit_tree = tree.fit(boot_x, boot_y)    estimators.append(fit_tree)    bagging_train_arr.append(tree.predict(x_train))    bagging_test_arr.append(tree.predict(x_test))Construct dataframes with all the bootstrapped data:# trainbagging_train = pd.DataFrame()for i in range(len(bagging_train_arr)):    col_name = &quot;Bootstrap Model &quot; + str(i + 1)    bagging_train[col_name] = bagging_train_arr[i]# testbagging_test = pd.DataFrame()for i in range(len(bagging_test_arr)):    col_name = &quot;Bootstrap Model &quot; + str(i + 1)    bagging_test[col_name] = bagging_test_arr[i]    # generate renaming row objrename = {}for i in range(0, 1104):    rename[i] = &quot;Training Row &quot; + str(i + 1)bagging_train.rename(rename, inplace=True)bagging_test.rename(rename,  inplace=True)Combine predictions from all the bootstraps and assess how the model performs:# combining all data points from the data to determine accuracyy_preds_train = []y_preds_test = []for row in bagging_train.iterrows():    if np.mean(row[1]) &amp;gt; 0.5:        y_preds_train.append(1)    else:        y_preds_train.append(0)for row in bagging_test.iterrows():    if np.mean(row[1]) &amp;gt; 0.5:        y_preds_test.append(1)    else:        y_preds_test.append(0)        def compare_acc(preds, actual):    count = 0    for i in range(len(preds)):        if preds[i] == actual.item(i):            count += 1    return(count/len(preds))bagging_train_score = compare_acc(y_preds_train,y_train)bagging_test_score = compare_acc(y_preds_test,y_test)print(&quot;[Bagging] Classification accuracy for train set: &quot;, bagging_train_score)print(&quot;[Bagging] Classification accuracy for test set: &quot;, bagging_test_score)[Bagging] Classification accuracy for train set:  0.9370059288537549[Bagging] Classification accuracy for test set:  0.9150197628458498The model clearly performed better after using bootstrapped data to fit it. It has increased from 88% on the training data to 94.0%, and from 88.1% on the test data to 90.4%. This makes bagging the most accurate model we have tried so far.Random ForestOur next ensemble method is random forest, which randomly subsets predictors upon which to generate decision trees.# config parametersnum_trees = 45new_depth = 6# model random forestmodel_rf = RandomForestClassifier(n_estimators=num_trees, max_depth=new_depth)# fit model on X_train datamodel_rf.fit(x_train, y_train)# predict using modely_pred_train_rf = model_rf.predict(x_train)y_pred_test_rf = model_rf.predict(x_test)# accuracy from train and testtrain_score_rf = accuracy_score(y_train, y_pred_train_rf)test_score_rf = accuracy_score(y_test, y_pred_test_rf)# print accuracy scoresprint(&quot;[Random Forest] Classification accuracy for train set: &quot;, train_score_rf)print(&quot;[Random Forest] Classification accuracy for test set:&quot;, test_score_rf)[Random Forest] Classification accuracy for train set:  0.9300889328063241[Random Forest] Classification accuracy for test set: 0.9229249011857708A random forest, at the same depth as the decision tree (namely a depth of 6) performs even better. The test data reaches an accuracy of about 92.6% in the training at 91.5% in the test.BoostingFinally, we will consider boosting, an iterative approach that might eliminate some more of the error in our trees.# define classifier functiondef boostingClassifier(x_train, y_train, depth):    # AdaBoostClassifier    abc = AdaBoostClassifier(DecisionTreeClassifier(max_depth=depth),                         n_estimators=800, learning_rate = 0.05)    abc.fit(x_train, y_train)    # staged_score train to plot    abc_predicts_train = list(abc.staged_score(x_train,y_train))    plt.plot(abc_predicts_train, label = &quot;train&quot;);    # staged_score test to plot    abc_predicts_test = list(abc.staged_score(x_test,y_test))    plt.plot(abc_predicts_test, label = &quot;test&quot;);    plt.legend()    plt.title(&quot;AdaBoost Classifier Accuracy, n = &quot;+str(depth))    plt.xlabel(&quot;Iterations&quot;)    plt.show()        return(&quot;Maximum test accuracy for depth of &quot;+str(depth)+&quot; is &quot;+str(max(abc_predicts_test))+&quot; at &quot;+str(abc_predicts_test.index(max(abc_predicts_test)))+&quot; iterations&quot;)for i in range(1,5):    print(boostingClassifier(x_train, y_train, i))Maximum test accuracy for depth of 1 is 0.9150197628458498 at 773 iterationsMaximum test accuracy for depth of 2 is 0.9298418972332015 at 751 iterationsMaximum test accuracy for depth of 3 is 0.9268774703557312 at 500 iterationsMaximum test accuracy for depth of 4 is 0.9219367588932806 at 530 iterationsWe see based upon an AdaBoostClassifier the maximum test accuracy of 93.0% is attained at a depth of 2. This is attained after 751 iterations. The AdaBoostClassifier is our most accurate model so far.Neural NetworksFinally, we created an artificial neural network to classify our playlist songs.# check input and output dimensionsinput_dim_2 = x_train.shape[1]output_dim_2 = 1print(input_dim_2,output_dim_2)14 1# create sequential multi-layer perceptronmodel2 = Sequential() # initial layermodel2.add(Dense(10, input_dim=input_dim_2,                  activation='relu')) # second layermodel2.add(Dense(10, input_dim=input_dim_2,                  activation='relu'))# third layermodel2.add(Dense(10, input_dim=input_dim_2,                  activation='relu'))# output layermodel2.add(Dense(1, activation='sigmoid'))# compile the modelmodel2.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])model2.summary()# fit the modelmodel2_history = model2.fit(    x_train, y_train,    epochs=50, validation_split = 0.5, batch_size = 128, verbose=False)# model lossprint(&quot;[Neural Net - Model 1] Loss: &quot;, model2_history.history['loss'][-1])print(&quot;[Neural Net - Model 1] Val Loss: &quot;, model2_history.history['val_loss'][-1])print(&quot;[Neural Net - Model 1] Test Loss: &quot;, model2.evaluate(x_test, y_test, verbose=False))print(&quot;[Neural Net - Model 1] Accuracy: &quot;, model2_history.history['acc'][-1])print(&quot;[Neural Net - Model 1] Val Accuracy: &quot;, model2_history.history['val_acc'][-1])[Neural Net - Model 1] Loss:  7.79790558079957[Neural Net - Model 1] Val Loss:  8.034205742033103[Neural Net - Model 1] Test Loss:  [7.719139232937055, 0.5158102769154334][Neural Net - Model 1] Accuracy:  0.5108695654529828[Neural Net - Model 1] Val Accuracy:  0.49604743024106085Our initial accuracy isn’t great. We achieve an accuracy of 48.9% in the training and 50.4% in the validation, and an accuracy of 48.4% in the test. Let’s see if we can improve our network to fit the data better.# create sequential multi-layer perceptronmodel3 = Sequential() # Hidden layersfor i in range(40):    model3.add(Dense(10, input_dim=input_dim_2,         activation='relu')) # output layermodel3.add(Dense(output_dim_2, activation='sigmoid'))# compile the modelmodel3.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])model3.summary()# fit the modelmodel3_history = model3.fit(    x_train, y_train,    epochs=300, validation_split = 0.1, batch_size = 128, verbose=False)# model lossprint(&quot;[Neural Net - Model 2] Loss: &quot;, model3_history.history['loss'][-1])print(&quot;[Neural Net - Model 2] Val Loss: &quot;, model3_history.history['val_loss'][-1])print(&quot;[Neural Net - Model 2] Test Loss: &quot;, model3.evaluate(x_test, y_test, verbose=False))print(&quot;[Neural Net - Model 2] Accuracy: &quot;, model3_history.history['acc'][-1])print(&quot;[Neural Net - Model 2] Val Accuracy: &quot;, model3_history.history['val_acc'][-1])[Neural Net - Model 2] Loss:  0.6267417644590524[Neural Net - Model 2] Val Loss:  0.6291195959220698[Neural Net - Model 2] Test Loss:  [0.6115785545040026, 0.6432806319398843][Neural Net - Model 2] Accuracy:  0.625857809154077[Neural Net - Model 2] Val Accuracy:  0.6197530875971288Even after changing hyperparameters, our neural network does not perform very well. Using 40 layers and 300 epochs, the accuracy in the training data is still 62.8% while the accuracy in the test is 65.2%. This is baffling, because we expected our neural network to perform very well. Perhaps this mediocre perforance is due to limitations of our data set (only 14 features and &amp;lt;5000 songs), or of the specific methods we used.Model SelectionBased upon the presented analysis, we conclude that our boosted decision tree classifier, at a depth of 2 with 751 iterations, is the best model. It achieves the highest accuracy in the test set, of 93.0%.",
    "url": "http://localhost:4000/final_notebook/models.html",
    "relUrl": "/final_notebook/models.html"
  }
}
